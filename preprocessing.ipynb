{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0632d2",
   "metadata": {},
   "source": [
    "# Pipeline de Preprocessing de Texte pour l'Analyse de Sentiments\n",
    "\n",
    "Ce notebook contient un pipeline complet de preprocessing de texte optimisé pour l'analyse de commentaires Reddit. Le pipeline comprend plusieurs étapes de nettoyage, tokenisation, suppression des mots vides et lemmatisation.\n",
    "\n",
    "## 📊 Chargement des Données\n",
    "\n",
    "Cette section charge le dataset de commentaires Reddit ChatGPT et configure le répertoire pour les données NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d42a42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ntlk_dir = 'nltk_data'\n",
    "df = pd.read_csv('data/chatgpt-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa95a15",
   "metadata": {},
   "source": [
    "## 🧹 Classe TextCleaner - Nettoyage du Texte\n",
    "\n",
    "Cette classe effectue le nettoyage et la préprocessing du texte avec plusieurs étapes :\n",
    "\n",
    "- **Suppression HTML** : Retire les balises HTML (`<p>`, `<div>`, etc.)\n",
    "- **Conversion en minuscules** : Uniformise la casse\n",
    "- **Suppression des URLs** : Retire les liens HTTP/HTTPS\n",
    "- **Suppression des mentions/hashtags** : Retire @mentions et #hashtags\n",
    "- **Suppression de la ponctuation** : Retire tous les caractères de ponctuation\n",
    "- **Suppression des chiffres** : Retire les nombres\n",
    "- **Normalisation des espaces** : Uniformise les espaces multiples\n",
    "\n",
    "La méthode `clean_text()` applique toutes ces étapes en séquence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "636445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaner:\n",
    "\t\"\"\"\n",
    "\tClass for cleaning and preprocessing text data with multiple processing steps.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef remove_html(self, text) -> str:\n",
    "\t\t\"\"\"Remove HTML tags from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\n",
    "\tdef convert_to_lowercase(self, text) -> str:\n",
    "\t\t\"\"\"Convert text to lowercase.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn text.lower()\n",
    "\t\n",
    "\tdef remove_urls(self, text) -> str:\n",
    "\t\t\"\"\"Remove URLs from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"http\\S+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_mentions_hashtags(self, text) -> str:\n",
    "\t\t\"\"\"Remove mentions (@username) and hashtags (#hashtag) from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_punctuation(self, text) -> str:\n",
    "\t\t\"\"\"Remove punctuation from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\n",
    "\tdef remove_digits(self, text) -> str:\n",
    "\t\t\"\"\"Remove digits from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\d+\", \"\", text)\n",
    "\t\n",
    "\tdef normalize_whitespace(self, text) -> str:\n",
    "\t\t\"\"\"Normalize whitespace in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\t\n",
    "\tdef clean_text(self, text) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tApply all cleaning steps to the text.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text to clean\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned text\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Apply all cleaning steps in sequence\n",
    "\t\ttext = self.remove_html(text)\n",
    "\t\ttext = self.convert_to_lowercase(text)\n",
    "\t\ttext = self.remove_urls(text)\n",
    "\t\ttext = self.remove_mentions_hashtags(text)\n",
    "\t\ttext = self.remove_punctuation(text)\n",
    "\t\ttext = self.remove_digits(text)\n",
    "\t\ttext = self.normalize_whitespace(text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "\treturn cleaner.clean_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d51c8",
   "metadata": {},
   "source": [
    "## 🔤 Classe TextTokenizer - Tokenisation\n",
    "\n",
    "Cette classe divise le texte nettoyé en tokens (mots individuels) en utilisant le tokenizer de NLTK.\n",
    "\n",
    "**Configuration NLTK** :\n",
    "- Configure le chemin de données NLTK vers le dossier local\n",
    "- Télécharge le package `punkt_tab` nécessaire pour la tokenisation\n",
    "\n",
    "**Fonctionnalité** :\n",
    "- Transforme une chaîne de caractères en liste de mots\n",
    "- Gère les contractions et la ponctuation restante\n",
    "- Retourne une liste vide si l'entrée n'est pas valide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdd1bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Configure NLTK data path\n",
    "nltk.data.path.append(os.path.abspath(ntlk_dir))\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('punkt_tab', download_dir=ntlk_dir)\n",
    "\n",
    "class TextTokenizer:\n",
    "\t\"\"\"\n",
    "\tClass for tokenizing cleaned text into word-level tokens.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef tokenize(self, text) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tTokenize text into words.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The cleaned input text\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: List of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\treturn word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7345d4",
   "metadata": {},
   "source": [
    "## 🚫 Classe StopwordRemover - Suppression des Mots Vides\n",
    "\n",
    "Cette classe supprime les mots vides (stopwords) qui n'apportent pas de sens sémantique significatif.\n",
    "\n",
    "**Caractéristiques** :\n",
    "- Utilise la liste de stopwords anglais de NLTK\n",
    "- **Préservation des négations** : Garde les mots comme \"not\", \"no\", \"never\", \"n't\" car ils sont cruciaux pour l'analyse de sentiment\n",
    "- Paramètre de langue configurable (anglais par défaut)\n",
    "- Comparaison insensible à la casse\n",
    "\n",
    "**Objectif** : Réduire le bruit tout en préservant les indicateurs de sentiment importants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cea26699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('stopwords', download_dir=ntlk_dir)\n",
    "\n",
    "class StopwordRemover:\n",
    "\t\"\"\"\n",
    "\tClass for removing stopwords from tokenized text.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, language=\"english\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the stopword remover with a given language.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): Language of the stopwords (default is 'english')\n",
    "\t\t\"\"\"\n",
    "\t\tself.stop_words = set(stopwords.words(language))\n",
    "\t\n",
    "\tdef remove_stopwords(self, tokens, keep_negation=True) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tRemove stopwords from a list of tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Tokens without stopwords\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\tif keep_negation:\n",
    "\t\t\tnegations = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words or word.lower() in negations]\n",
    "\t\telse:\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0463c5",
   "metadata": {},
   "source": [
    "## 🔄 Classe TextLemmatizer - Lemmatisation\n",
    "\n",
    "Cette classe réduit les mots à leur forme canonique (lemme) pour normaliser les variations morphologiques.\n",
    "\n",
    "**Packages NLTK requis** :\n",
    "- `wordnet` : Base de données lexicale\n",
    "- `omw-1.4` : Open Multilingual Wordnet\n",
    "- `averaged_perceptron_tagger_eng` : Étiqueteur morpho-syntaxique anglais\n",
    "\n",
    "**Fonctionnement** :\n",
    "1. **POS Tagging** : Détermine la catégorie grammaticale de chaque mot\n",
    "2. **Conversion des tags** : Convertit les tags TreeBank vers le format WordNet\n",
    "3. **Lemmatisation** : Applique la lemmatisation en fonction du type grammatical\n",
    "\n",
    "**Exemples** : \"running\" → \"run\", \"better\" → \"good\", \"children\" → \"child\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb654ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('wordnet', download_dir=ntlk_dir)\n",
    "# nltk.download('omw-1.4', download_dir=ntlk_dir)\n",
    "# nltk.download('averaged_perceptron_tagger_eng', download_dir=ntlk_dir)\n",
    "\n",
    "class TextLemmatizer:\n",
    "\t\"\"\"\n",
    "\tClass for lemmatizing word tokens using NLTK's WordNetLemmatizer.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\t\n",
    "\tdef get_wordnet_pos(self, treebank_tag) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tConvert POS tag from Treebank to WordNet format for better lemmatization.\n",
    "\t\t\"\"\"\n",
    "\t\tif treebank_tag.startswith('J'):\n",
    "\t\t\treturn wordnet.ADJ\n",
    "\t\telif treebank_tag.startswith('V'):\n",
    "\t\t\treturn wordnet.VERB\n",
    "\t\telif treebank_tag.startswith('N'):\n",
    "\t\t\treturn wordnet.NOUN\n",
    "\t\telif treebank_tag.startswith('R'):\n",
    "\t\t\treturn wordnet.ADV\n",
    "\t\telse:\n",
    "\t\t\treturn wordnet.NOUN  # fallback\n",
    "\t\n",
    "\tdef lemmatize(self, tokens) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize a list of word tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Lemmatized tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)  # POS tagging\n",
    "\t\treturn [\n",
    "\t\t\tself.lemmatizer.lemmatize(token, self.get_wordnet_pos(pos))\n",
    "\t\t\tfor token, pos in pos_tags\n",
    "\t\t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b7d38",
   "metadata": {},
   "source": [
    "## 🧪 Démonstration du Pipeline Complet\n",
    "\n",
    "Cette section teste le pipeline complet sur des exemples variés pour illustrer chaque étape du preprocessing.\n",
    "\n",
    "**Exemples de test** :\n",
    "1. **HTML + Mentions/URLs** : Texte avec balises, mentions et liens\n",
    "2. **Expressions informelles** : Contractions et ponctuation excessive  \n",
    "3. **Négations** : Test de préservation des mots négatifs\n",
    "4. **Contenu web** : Hashtags et URLs de sites web\n",
    "5. **Formes grammaticales** : Différentes conjugaisons et pluriels\n",
    "\n",
    "**Étapes visualisées** :\n",
    "- Texte original → Texte nettoyé → Tokens → Sans stopwords → Lemmatisé\n",
    "\n",
    "Cela permet de vérifier l'efficacité de chaque composant du pipeline.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎛️ Approche 1 : Configuration par Paramètres\n",
    "\n",
    "Cette approche utilise un dictionnaire de configuration pour contrôler finement chaque étape du preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import emoji\n",
    "\n",
    "class FlexibleTextProcessor:\n",
    "\t\"\"\"\n",
    "\tFlexible text processor with configurable cleaning levels.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, config: Dict[str, Any] = None):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize with configuration dictionary.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tconfig: Dictionary with processing options\n",
    "\t\t\"\"\"\n",
    "\t\t# Default configuration (Light cleaning)\n",
    "\t\tself.default_config = {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': False,  # Keep hashtags for sentiment context\n",
    "\t\t\t'remove_punctuation': False,  # Keep punctuation for emphasis\n",
    "\t\t\t'remove_digits': False,  # Keep numbers for context\n",
    "\t\t\t'remove_emojis': False,  # Keep emojis for sentiment\n",
    "\t\t\t'expand_contractions': False,  # Keep natural speech patterns\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': False,  # Avoid over-normalization\n",
    "\t\t\t'preserve_caps': False,  # Keep CAPS for emphasis\n",
    "\t\t\t'min_word_length': 1,  # Keep short words like \"I\", \"no\"\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tself.config = self.default_config.copy()\n",
    "\t\tif config:\n",
    "\t\t\tself.config.update(config)\n",
    "\t\t\n",
    "\t\t# Initialize components\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\t\n",
    "\t\t# Contractions dictionary\n",
    "\t\tself.contractions = {\n",
    "\t\t\t\"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "\t\t\t\"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
    "\t\t\t\"'m\": \" am\", \"'s\": \" is\"\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_light_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Light cleaning configuration - minimal processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': False,\n",
    "\t\t\t'remove_punctuation': False,\n",
    "\t\t\t'remove_digits': False,\n",
    "\t\t\t'remove_emojis': False,\n",
    "\t\t\t'expand_contractions': False,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': False,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 1,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_medium_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Medium cleaning configuration - balanced processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': True,\n",
    "\t\t\t'remove_punctuation': True,\n",
    "\t\t\t'remove_digits': True,\n",
    "\t\t\t'remove_emojis': False,  # Keep emojis\n",
    "\t\t\t'expand_contractions': True,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': True,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 2,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_hard_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Hard cleaning configuration - aggressive processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': True,\n",
    "\t\t\t'remove_punctuation': True,\n",
    "\t\t\t'remove_digits': True,\n",
    "\t\t\t'remove_emojis': True,\n",
    "\t\t\t'expand_contractions': True,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': True,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 3,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef expand_contractions(self, text: str) -> str:\n",
    "\t\t\"\"\"Expand contractions in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\tfor contraction, expansion in self.contractions.items():\n",
    "\t\t\ttext = text.replace(contraction, expansion)\n",
    "\t\treturn text\n",
    "\t\n",
    "\tdef remove_emojis(self, text: str) -> str:\n",
    "\t\t\"\"\"Remove emojis from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn emoji.replace_emoji(text, replace='')\n",
    "\t\n",
    "\tdef preserve_emphasis(self, text: str) -> str:\n",
    "\t\t\"\"\"Convert emphasis patterns to tokens.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Convert multiple punctuation to emphasis tokens\n",
    "\t\ttext = re.sub(r'!{2,}', ' VERY_EXCITED ', text)\n",
    "\t\ttext = re.sub(r'\\?{2,}', ' VERY_QUESTIONING ', text)\n",
    "\t\ttext = re.sub(r'\\.{3,}', ' TRAILING_THOUGHT ', text)\n",
    "\t\t\n",
    "\t\t# Convert CAPS words to emphasis tokens\n",
    "\t\tif not self.config.get('preserve_caps', False):\n",
    "\t\t\ttext = re.sub(r'\\b[A-Z]{2,}\\b', lambda m: f'EMPHASIS_{m.group().lower()}', text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\t\n",
    "\tdef process_text(self, text: str) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tProcess text according to configuration.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext: Input text to process\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tProcessed text string\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\tresult = text\n",
    "\t\t\n",
    "\t\t# Step 1: HTML removal\n",
    "\t\tif self.config.get('remove_html', True):\n",
    "\t\t\tresult = BeautifulSoup(result, \"html.parser\").get_text()\n",
    "\t\t\n",
    "\t\t# Step 2: Preserve emphasis before other processing\n",
    "\t\tif not self.config.get('remove_punctuation', True):\n",
    "\t\t\tresult = self.preserve_emphasis(result)\n",
    "\t\t\n",
    "\t\t# Step 3: Expand contractions\n",
    "\t\tif self.config.get('expand_contractions', False):\n",
    "\t\t\tresult = self.expand_contractions(result)\n",
    "\t\t\n",
    "\t\t# Step 4: Case handling\n",
    "\t\tif self.config.get('to_lowercase', True):\n",
    "\t\t\tif not self.config.get('preserve_caps', False):\n",
    "\t\t\t\tresult = result.lower()\n",
    "\t\t\n",
    "\t\t# Step 5: Remove various elements\n",
    "\t\tif self.config.get('remove_urls', True):\n",
    "\t\t\tresult = re.sub(r\"http\\S+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_mentions', True):\n",
    "\t\t\tresult = re.sub(r\"@\\w+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_hashtags', False):\n",
    "\t\t\tresult = re.sub(r\"#\\w+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_emojis', False):\n",
    "\t\t\tresult = self.remove_emojis(result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_punctuation', False):\n",
    "\t\t\tresult = re.sub(r\"[^\\w\\s]\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_digits', False):\n",
    "\t\t\tresult = re.sub(r\"\\d+\", \"\", result)\n",
    "\t\t\n",
    "\t\t# Step 6: Normalize whitespace\n",
    "\t\tif self.config.get('normalize_whitespace', True):\n",
    "\t\t\tresult = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "\t\t\n",
    "\t\treturn result\n",
    "\t\n",
    "\tdef process_tokens(self, tokens: list[str]) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tProcess tokens according to configuration.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens: List of tokens\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tProcessed list of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\t\n",
    "\t\tresult = tokens.copy()\n",
    "\t\t\n",
    "\t\t# Filter by minimum word length\n",
    "\t\tmin_length = self.config.get('min_word_length', 1)\n",
    "\t\tresult = [token for token in result if len(token) >= min_length]\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\tif self.config.get('remove_stopwords', True):\n",
    "\t\t\tkeep_neg = self.config.get('keep_negations', True)\n",
    "\t\t\tresult = self.stopword_remover.remove_stopwords(result, keep_negation=keep_neg)\n",
    "\t\t\n",
    "\t\t# Apply lemmatization\n",
    "\t\tif self.config.get('apply_lemmatization', False):\n",
    "\t\t\tresult = self.lemmatizer.lemmatize(result)\n",
    "\t\t\n",
    "\t\treturn result\n",
    "\t\n",
    "\tdef full_pipeline(self, text: str) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tComplete processing pipeline.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext: Input text\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList of processed tokens\n",
    "\t\t\"\"\"\n",
    "\t\t# Process text\n",
    "\t\tcleaned_text = self.process_text(text)\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned_text)\n",
    "\t\t\n",
    "\t\t# Process tokens\n",
    "\t\tfinal_tokens = self.process_tokens(tokens)\n",
    "\t\t\n",
    "\t\treturn final_tokens\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎯 Approche 2 : Classes Spécialisées (Enum-based)\n",
    "\n",
    "Cette approche utilise des enums et des classes spécialisées pour chaque niveau de nettoyage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CleaningLevel(Enum):\n",
    "\tLIGHT = \"light\"\n",
    "\tMEDIUM = \"medium\"\n",
    "\tHARD = \"hard\"\n",
    "\tCUSTOM = \"custom\"\n",
    "\n",
    "class BaseProcessor(ABC):\n",
    "\t\"\"\"Abstract base class for text processors.\"\"\"\n",
    "\t\n",
    "\t@abstractmethod\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\tpass\n",
    "\n",
    "class LightProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tLight cleaning - Preserve sentiment indicators\n",
    "\t- Keep punctuation for emphasis (!!!, ???)\n",
    "\t- Keep emojis for emotional context\n",
    "\t- Keep contractions for natural speech\n",
    "\t- Minimal lemmatization to preserve meaning nuances\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Custom light cleaning\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\t\n",
    "\t\t# Remove only essential noise\n",
    "\t\tresult = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\tresult = result.lower()\n",
    "\t\tresult = re.sub(r\"http\\S+\", \"\", result)  # Remove URLs\n",
    "\t\tresult = re.sub(r\"@\\w+\", \"\", result)   # Remove mentions\n",
    "\t\t\n",
    "\t\t# Keep hashtags, punctuation, emojis, numbers\n",
    "\t\tresult = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(result)\n",
    "\t\t\n",
    "\t\t# Light stopword removal (keep negations)\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# No lemmatization to preserve word forms\n",
    "\t\treturn tokens\n",
    "\n",
    "class MediumProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tMedium cleaning - Balanced approach\n",
    "\t- Remove most punctuation but preserve key patterns\n",
    "\t- Selective emoji handling\n",
    "\t- Moderate lemmatization\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Standard cleaning\n",
    "\t\tcleaned = self.text_cleaner.clean_text(text)\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned)\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# Selective lemmatization (avoid over-normalization)\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)\n",
    "\t\tlemmatized = []\n",
    "\t\t\n",
    "\t\tfor token, pos in pos_tags:\n",
    "\t\t\t# Don't lemmatize adjectives to preserve sentiment intensity\n",
    "\t\t\tif pos.startswith('JJ'):  # Adjectives (better, worse, etc.)\n",
    "\t\t\t\tlemmatized.append(token)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemmatized.append(self.lemmatizer.lemmatizer.lemmatize(\n",
    "\t\t\t\t\ttoken, self.lemmatizer.get_wordnet_pos(pos)\n",
    "\t\t\t\t))\n",
    "\t\t\n",
    "\t\treturn lemmatized\n",
    "\n",
    "class HardProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tHard cleaning - Aggressive normalization\n",
    "\t- Remove all non-essential elements\n",
    "\t- Full lemmatization\n",
    "\t- Strict filtering\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Aggressive cleaning\n",
    "\t\tcleaned = self.text_cleaner.clean_text(text)\n",
    "\t\t\n",
    "\t\t# Additional aggressive steps\n",
    "\t\tcleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)  # Remove all punctuation\n",
    "\t\tcleaned = emoji.replace_emoji(cleaned, replace='')  # Remove emojis\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned)\n",
    "\t\t\n",
    "\t\t# Filter short tokens\n",
    "\t\ttokens = [token for token in tokens if len(token) >= 3]\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# Full lemmatization\n",
    "\t\ttokens = self.lemmatizer.lemmatize(tokens)\n",
    "\t\t\n",
    "\t\treturn tokens\n",
    "\n",
    "class ProcessorFactory:\n",
    "\t\"\"\"Factory to create appropriate processor based on cleaning level.\"\"\"\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef create_processor(level: CleaningLevel) -> BaseProcessor:\n",
    "\t\t\"\"\"Create processor based on cleaning level.\"\"\"\n",
    "\t\tif level == CleaningLevel.LIGHT:\n",
    "\t\t\treturn LightProcessor()\n",
    "\t\telif level == CleaningLevel.MEDIUM:\n",
    "\t\t\treturn MediumProcessor()\n",
    "\t\telif level == CleaningLevel.HARD:\n",
    "\t\t\treturn HardProcessor()\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Unsupported cleaning level: {level}\")\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef process_text(text: str, level: CleaningLevel) -> list[str]:\n",
    "\t\t\"\"\"Quick processing with specified level.\"\"\"\n",
    "\t\tprocessor = ProcessorFactory.create_processor(level)\n",
    "\t\treturn processor.process(text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🧪 Démonstration des Différentes Approches\n",
    "\n",
    "Comparaison des résultats entre les différents niveaux de nettoyage sur les mêmes exemples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2e2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92c8aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMONSTRATION OF THE PREPROCESSING ===\n",
      "\n",
      "📝 Exemple 1:\n",
      "Original: <p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool 🚀</p>\n",
      "Cleaned: 'hello check out this amazing tool'\n",
      "Tokens: ['hello', 'check', 'out', 'this', 'amazing', 'tool']\n",
      "Without stopwords: ['hello', 'check', 'amazing', 'tool']\n",
      "Lemmatized: ['hello', 'check', 'amaze', 'tool']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 Exemple 2:\n",
      "Original: I'm loving the new ChatGPT updates!!! It's so much better than before... 😍\n",
      "Cleaned: 'im loving the new chatgpt updates its so much better than before'\n",
      "Tokens: ['im', 'loving', 'the', 'new', 'chatgpt', 'updates', 'its', 'so', 'much', 'better', 'than', 'before']\n",
      "Without stopwords: ['im', 'loving', 'new', 'chatgpt', 'updates', 'much', 'better']\n",
      "Lemmatized: ['im', 'love', 'new', 'chatgpt', 'update', 'much', 'good']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 Exemple 3:\n",
      "Original: Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\n",
      "Cleaned: 'why are people still using old technologies in makes no sense to me'\n",
      "Tokens: ['why', 'are', 'people', 'still', 'using', 'old', 'technologies', 'in', 'makes', 'no', 'sense', 'to', 'me']\n",
      "Without stopwords: ['people', 'still', 'using', 'old', 'technologies', 'makes', 'no', 'sense']\n",
      "Lemmatized: ['people', 'still', 'use', 'old', 'technology', 'make', 'no', 'sense']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 Exemple 4:\n",
      "Original: <div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\n",
      "Cleaned: 'visit our website wwwexamplecom for more info about and'\n",
      "Tokens: ['visit', 'our', 'website', 'wwwexamplecom', 'for', 'more', 'info', 'about', 'and']\n",
      "Without stopwords: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "Lemmatized: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 Exemple 5:\n",
      "Original: Running, jumped, better, good, children, mice, feet - testing different word forms\n",
      "Cleaned: 'running jumped better good children mice feet testing different word forms'\n",
      "Tokens: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Without stopwords: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Lemmatized: ['run', 'jump', 'good', 'good', 'child', 'mice', 'foot', 'test', 'different', 'word', 'form']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# Create the instances of the classes\n",
    "text_cleaner = TextCleaner()\n",
    "tokenizer = TextTokenizer()\n",
    "stopword_remover = StopwordRemover()\n",
    "lemmatizer = TextLemmatizer()\n",
    "\n",
    "# Examples of texts with different problems\n",
    "sample_texts = [\n",
    "\t\"<p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool 🚀</p>\",\n",
    "\t\"I'm loving the new ChatGPT updates!!! It's so much better than before... 😍\",\n",
    "\t\"Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\",\n",
    "\t\"<div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\",\n",
    "\t\"Running, jumped, better, good, children, mice, feet - testing different word forms\"\n",
    "]\n",
    "\n",
    "print(\"=== DEMONSTRATION OF THE PREPROCESSING ===\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "\tprint(f\"📝 Exemple {i}:\")\n",
    "\tprint(f\"Original: {text}\")\n",
    "\t\n",
    "\t# Step 1: Cleaning\n",
    "\tcleaned = text_cleaner.clean_text(text)\n",
    "\tprint(f\"Cleaned: '{cleaned}'\")\n",
    "\t\n",
    "\t# Step 2: Tokenization\n",
    "\ttokens = tokenizer.tokenize(cleaned)\n",
    "\tprint(f\"Tokens: {tokens}\")\n",
    "\t\n",
    "\t# Step 3: Stopword Removal\n",
    "\ttokens_no_stop = stopword_remover.remove_stopwords(tokens)\n",
    "\tprint(f\"Without stopwords: {tokens_no_stop}\")\n",
    "\t\n",
    "\t# Step 4: Lemmatization\n",
    "\tlemmatized = lemmatizer.lemmatize(tokens_no_stop)\n",
    "\tprint(f\"Lemmatized: {lemmatized}\")\n",
    "\t\n",
    "\tprint(\"-\" * 80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
