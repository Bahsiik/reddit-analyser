{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0632d2",
   "metadata": {},
   "source": [
    "# Pipeline de Preprocessing de Texte pour l'Analyse de Sentiments\n",
    "\n",
    "Ce notebook contient un pipeline complet de preprocessing de texte optimis√© pour l'analyse de commentaires Reddit. Le pipeline comprend plusieurs √©tapes de nettoyage, tokenisation, suppression des mots vides et lemmatisation.\n",
    "\n",
    "## üìä Chargement des Donn√©es\n",
    "\n",
    "Cette section charge le dataset de commentaires Reddit ChatGPT et configure le r√©pertoire pour les donn√©es NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d42a42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ntlk_dir = 'nltk_data'\n",
    "df = pd.read_csv('data/chatgpt-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa95a15",
   "metadata": {},
   "source": [
    "## üßπ Classe TextCleaner - Nettoyage du Texte\n",
    "\n",
    "Cette classe effectue le nettoyage et la pr√©processing du texte avec plusieurs √©tapes :\n",
    "\n",
    "- **Suppression HTML** : Retire les balises HTML (`<p>`, `<div>`, etc.)\n",
    "- **Conversion en minuscules** : Uniformise la casse\n",
    "- **Suppression des URLs** : Retire les liens HTTP/HTTPS\n",
    "- **Suppression des mentions/hashtags** : Retire @mentions et #hashtags\n",
    "- **Suppression de la ponctuation** : Retire tous les caract√®res de ponctuation\n",
    "- **Suppression des chiffres** : Retire les nombres\n",
    "- **Normalisation des espaces** : Uniformise les espaces multiples\n",
    "\n",
    "La m√©thode `clean_text()` applique toutes ces √©tapes en s√©quence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "636445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaner:\n",
    "\t\"\"\"\n",
    "\tClass for cleaning and preprocessing text data with multiple processing steps.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef remove_html(self, text) -> str:\n",
    "\t\t\"\"\"Remove HTML tags from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\n",
    "\tdef convert_to_lowercase(self, text) -> str:\n",
    "\t\t\"\"\"Convert text to lowercase.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn text.lower()\n",
    "\t\n",
    "\tdef remove_urls(self, text) -> str:\n",
    "\t\t\"\"\"Remove URLs from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"http\\S+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_mentions_hashtags(self, text) -> str:\n",
    "\t\t\"\"\"Remove mentions (@username) and hashtags (#hashtag) from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_punctuation(self, text) -> str:\n",
    "\t\t\"\"\"Remove punctuation from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\n",
    "\tdef remove_digits(self, text) -> str:\n",
    "\t\t\"\"\"Remove digits from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\d+\", \"\", text)\n",
    "\t\n",
    "\tdef normalize_whitespace(self, text) -> str:\n",
    "\t\t\"\"\"Normalize whitespace in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\t\n",
    "\tdef clean_text(self, text) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tApply all cleaning steps to the text.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text to clean\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned text\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Apply all cleaning steps in sequence\n",
    "\t\ttext = self.remove_html(text)\n",
    "\t\ttext = self.convert_to_lowercase(text)\n",
    "\t\ttext = self.remove_urls(text)\n",
    "\t\ttext = self.remove_mentions_hashtags(text)\n",
    "\t\ttext = self.remove_punctuation(text)\n",
    "\t\ttext = self.remove_digits(text)\n",
    "\t\ttext = self.normalize_whitespace(text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "\treturn cleaner.clean_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d51c8",
   "metadata": {},
   "source": [
    "## üî§ Classe TextTokenizer - Tokenisation\n",
    "\n",
    "Cette classe divise le texte nettoy√© en tokens (mots individuels) en utilisant le tokenizer de NLTK.\n",
    "\n",
    "**Configuration NLTK** :\n",
    "- Configure le chemin de donn√©es NLTK vers le dossier local\n",
    "- T√©l√©charge le package `punkt_tab` n√©cessaire pour la tokenisation\n",
    "\n",
    "**Fonctionnalit√©** :\n",
    "- Transforme une cha√Æne de caract√®res en liste de mots\n",
    "- G√®re les contractions et la ponctuation restante\n",
    "- Retourne une liste vide si l'entr√©e n'est pas valide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdd1bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Configure NLTK data path\n",
    "nltk.data.path.append(os.path.abspath(ntlk_dir))\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('punkt_tab', download_dir=ntlk_dir)\n",
    "\n",
    "class TextTokenizer:\n",
    "\t\"\"\"\n",
    "\tClass for tokenizing cleaned text into word-level tokens.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef tokenize(self, text) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tTokenize text into words.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The cleaned input text\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: List of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\treturn word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7345d4",
   "metadata": {},
   "source": [
    "## üö´ Classe StopwordRemover - Suppression des Mots Vides\n",
    "\n",
    "Cette classe supprime les mots vides (stopwords) qui n'apportent pas de sens s√©mantique significatif.\n",
    "\n",
    "**Caract√©ristiques** :\n",
    "- Utilise la liste de stopwords anglais de NLTK\n",
    "- **Pr√©servation des n√©gations** : Garde les mots comme \"not\", \"no\", \"never\", \"n't\" car ils sont cruciaux pour l'analyse de sentiment\n",
    "- Param√®tre de langue configurable (anglais par d√©faut)\n",
    "- Comparaison insensible √† la casse\n",
    "\n",
    "**Objectif** : R√©duire le bruit tout en pr√©servant les indicateurs de sentiment importants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cea26699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('stopwords', download_dir=ntlk_dir)\n",
    "\n",
    "class StopwordRemover:\n",
    "\t\"\"\"\n",
    "\tClass for removing stopwords from tokenized text.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, language=\"english\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the stopword remover with a given language.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): Language of the stopwords (default is 'english')\n",
    "\t\t\"\"\"\n",
    "\t\tself.stop_words = set(stopwords.words(language))\n",
    "\t\n",
    "\tdef remove_stopwords(self, tokens, keep_negation=True) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tRemove stopwords from a list of tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Tokens without stopwords\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\tif keep_negation:\n",
    "\t\t\tnegations = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words or word.lower() in negations]\n",
    "\t\telse:\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0463c5",
   "metadata": {},
   "source": [
    "## üîÑ Classe TextLemmatizer - Lemmatisation\n",
    "\n",
    "Cette classe r√©duit les mots √† leur forme canonique (lemme) pour normaliser les variations morphologiques.\n",
    "\n",
    "**Packages NLTK requis** :\n",
    "- `wordnet` : Base de donn√©es lexicale\n",
    "- `omw-1.4` : Open Multilingual Wordnet\n",
    "- `averaged_perceptron_tagger_eng` : √âtiqueteur morpho-syntaxique anglais\n",
    "\n",
    "**Fonctionnement** :\n",
    "1. **POS Tagging** : D√©termine la cat√©gorie grammaticale de chaque mot\n",
    "2. **Conversion des tags** : Convertit les tags TreeBank vers le format WordNet\n",
    "3. **Lemmatisation** : Applique la lemmatisation en fonction du type grammatical\n",
    "\n",
    "**Exemples** : \"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\", \"children\" ‚Üí \"child\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb654ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('wordnet', download_dir=ntlk_dir)\n",
    "# nltk.download('omw-1.4', download_dir=ntlk_dir)\n",
    "# nltk.download('averaged_perceptron_tagger_eng', download_dir=ntlk_dir)\n",
    "\n",
    "class TextLemmatizer:\n",
    "\t\"\"\"\n",
    "\tClass for lemmatizing word tokens using NLTK's WordNetLemmatizer.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\t\n",
    "\tdef get_wordnet_pos(self, treebank_tag) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tConvert POS tag from Treebank to WordNet format for better lemmatization.\n",
    "\t\t\"\"\"\n",
    "\t\tif treebank_tag.startswith('J'):\n",
    "\t\t\treturn wordnet.ADJ\n",
    "\t\telif treebank_tag.startswith('V'):\n",
    "\t\t\treturn wordnet.VERB\n",
    "\t\telif treebank_tag.startswith('N'):\n",
    "\t\t\treturn wordnet.NOUN\n",
    "\t\telif treebank_tag.startswith('R'):\n",
    "\t\t\treturn wordnet.ADV\n",
    "\t\telse:\n",
    "\t\t\treturn wordnet.NOUN  # fallback\n",
    "\t\n",
    "\tdef lemmatize(self, tokens) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize a list of word tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Lemmatized tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)  # POS tagging\n",
    "\t\treturn [\n",
    "\t\t\tself.lemmatizer.lemmatize(token, self.get_wordnet_pos(pos))\n",
    "\t\t\tfor token, pos in pos_tags\n",
    "\t\t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b7d38",
   "metadata": {},
   "source": [
    "## üß™ D√©monstration du Pipeline Complet\n",
    "\n",
    "Cette section teste le pipeline complet sur des exemples vari√©s pour illustrer chaque √©tape du preprocessing.\n",
    "\n",
    "**Exemples de test** :\n",
    "1. **HTML + Mentions/URLs** : Texte avec balises, mentions et liens\n",
    "2. **Expressions informelles** : Contractions et ponctuation excessive  \n",
    "3. **N√©gations** : Test de pr√©servation des mots n√©gatifs\n",
    "4. **Contenu web** : Hashtags et URLs de sites web\n",
    "5. **Formes grammaticales** : Diff√©rentes conjugaisons et pluriels\n",
    "\n",
    "**√âtapes visualis√©es** :\n",
    "- Texte original ‚Üí Texte nettoy√© ‚Üí Tokens ‚Üí Sans stopwords ‚Üí Lemmatis√©\n",
    "\n",
    "Cela permet de v√©rifier l'efficacit√© de chaque composant du pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92c8aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âMONSTRATION DU PREPROCESSING ===\n",
      "\n",
      "üìù Exemple 1:\n",
      "Original: <p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\n",
      "Nettoy√©: 'hello check out this amazing tool'\n",
      "Tokens: ['hello', 'check', 'out', 'this', 'amazing', 'tool']\n",
      "Sans mots vides: ['hello', 'check', 'amazing', 'tool']\n",
      "Lemmatis√©: ['hello', 'check', 'amaze', 'tool']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 2:\n",
      "Original: I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\n",
      "Nettoy√©: 'im loving the new chatgpt updates its so much better than before'\n",
      "Tokens: ['im', 'loving', 'the', 'new', 'chatgpt', 'updates', 'its', 'so', 'much', 'better', 'than', 'before']\n",
      "Sans mots vides: ['im', 'loving', 'new', 'chatgpt', 'updates', 'much', 'better']\n",
      "Lemmatis√©: ['im', 'love', 'new', 'chatgpt', 'update', 'much', 'good']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 3:\n",
      "Original: Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\n",
      "Nettoy√©: 'why are people still using old technologies in makes no sense to me'\n",
      "Tokens: ['why', 'are', 'people', 'still', 'using', 'old', 'technologies', 'in', 'makes', 'no', 'sense', 'to', 'me']\n",
      "Sans mots vides: ['people', 'still', 'using', 'old', 'technologies', 'makes', 'no', 'sense']\n",
      "Lemmatis√©: ['people', 'still', 'use', 'old', 'technology', 'make', 'no', 'sense']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 4:\n",
      "Original: <div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\n",
      "Nettoy√©: 'visit our website wwwexamplecom for more info about and'\n",
      "Tokens: ['visit', 'our', 'website', 'wwwexamplecom', 'for', 'more', 'info', 'about', 'and']\n",
      "Sans mots vides: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "Lemmatis√©: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 5:\n",
      "Original: Running, jumped, better, good, children, mice, feet - testing different word forms\n",
      "Nettoy√©: 'running jumped better good children mice feet testing different word forms'\n",
      "Tokens: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Sans mots vides: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Lemmatis√©: ['run', 'jump', 'good', 'good', 'child', 'mice', 'foot', 'test', 'different', 'word', 'form']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation avec des cha√Ænes de caract√®res al√©atoires\n",
    "\n",
    "# Cr√©er les instances des classes\n",
    "text_cleaner = TextCleaner()\n",
    "tokenizer = TextTokenizer()\n",
    "stopword_remover = StopwordRemover()\n",
    "lemmatizer = TextLemmatizer()\n",
    "\n",
    "# Exemples de textes avec diff√©rents probl√®mes\n",
    "sample_texts = [\n",
    "\t\"<p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\",\n",
    "\t\"I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\",\n",
    "\t\"Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\",\n",
    "\t\"<div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\",\n",
    "\t\"Running, jumped, better, good, children, mice, feet - testing different word forms\"\n",
    "]\n",
    "\n",
    "print(\"=== D√âMONSTRATION DU PREPROCESSING ===\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "\tprint(f\"üìù Exemple {i}:\")\n",
    "\tprint(f\"Original: {text}\")\n",
    "\t\n",
    "\t# √âtape 1: Nettoyage\n",
    "\tcleaned = text_cleaner.clean_text(text)\n",
    "\tprint(f\"Nettoy√©: '{cleaned}'\")\n",
    "\t\n",
    "\t# √âtape 2: Tokenisation\n",
    "\ttokens = tokenizer.tokenize(cleaned)\n",
    "\tprint(f\"Tokens: {tokens}\")\n",
    "\t\n",
    "\t# √âtape 3: Suppression des mots vides\n",
    "\ttokens_no_stop = stopword_remover.remove_stopwords(tokens)\n",
    "\tprint(f\"Sans mots vides: {tokens_no_stop}\")\n",
    "\t\n",
    "\t# √âtape 4: Lemmatisation\n",
    "\tlemmatized = lemmatizer.lemmatize(tokens_no_stop)\n",
    "\tprint(f\"Lemmatis√©: {lemmatized}\")\n",
    "\t\n",
    "\tprint(\"-\" * 80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
