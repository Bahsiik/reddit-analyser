{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0632d2",
   "metadata": {},
   "source": [
    "# Pipeline de Preprocessing de Texte pour l'Analyse de Sentiments\n",
    "\n",
    "Ce notebook contient un pipeline complet de preprocessing de texte optimis√© pour l'analyse de commentaires Reddit. Le pipeline comprend plusieurs √©tapes de nettoyage, tokenisation, suppression des mots vides et lemmatisation.\n",
    "\n",
    "## üìä Chargement des Donn√©es\n",
    "\n",
    "Cette section charge le dataset de commentaires Reddit ChatGPT et configure le r√©pertoire pour les donn√©es NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d42a42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ntlk_dir = 'nltk_data'\n",
    "df = pd.read_csv('data/chatgpt-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa95a15",
   "metadata": {},
   "source": [
    "## üßπ Classe TextCleaner - Nettoyage du Texte\n",
    "\n",
    "Cette classe effectue le nettoyage et la pr√©processing du texte avec plusieurs √©tapes :\n",
    "\n",
    "- **Suppression HTML** : Retire les balises HTML (`<p>`, `<div>`, etc.)\n",
    "- **Conversion en minuscules** : Uniformise la casse\n",
    "- **Suppression des URLs** : Retire les liens HTTP/HTTPS\n",
    "- **Suppression des mentions/hashtags** : Retire @mentions et #hashtags\n",
    "- **Suppression de la ponctuation** : Retire tous les caract√®res de ponctuation\n",
    "- **Suppression des chiffres** : Retire les nombres\n",
    "- **Normalisation des espaces** : Uniformise les espaces multiples\n",
    "\n",
    "La m√©thode `clean_text()` applique toutes ces √©tapes en s√©quence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "636445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaner:\n",
    "\t\"\"\"\n",
    "\tClass for cleaning and preprocessing text data with multiple processing steps.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef remove_html(self, text) -> str:\n",
    "\t\t\"\"\"Remove HTML tags from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\n",
    "\tdef convert_to_lowercase(self, text) -> str:\n",
    "\t\t\"\"\"Convert text to lowercase.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn text.lower()\n",
    "\t\n",
    "\tdef remove_urls(self, text) -> str:\n",
    "\t\t\"\"\"Remove URLs from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"http\\S+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_mentions_hashtags(self, text) -> str:\n",
    "\t\t\"\"\"Remove mentions (@username) and hashtags (#hashtag) from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_punctuation(self, text) -> str:\n",
    "\t\t\"\"\"Remove punctuation from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\n",
    "\tdef remove_digits(self, text) -> str:\n",
    "\t\t\"\"\"Remove digits from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\d+\", \"\", text)\n",
    "\t\n",
    "\tdef normalize_whitespace(self, text) -> str:\n",
    "\t\t\"\"\"Normalize whitespace in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\t\n",
    "\tdef clean_text(self, text) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tApply all cleaning steps to the text.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text to clean\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned text\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Apply all cleaning steps in sequence\n",
    "\t\ttext = self.remove_html(text)\n",
    "\t\ttext = self.convert_to_lowercase(text)\n",
    "\t\ttext = self.remove_urls(text)\n",
    "\t\ttext = self.remove_mentions_hashtags(text)\n",
    "\t\ttext = self.remove_punctuation(text)\n",
    "\t\ttext = self.remove_digits(text)\n",
    "\t\ttext = self.normalize_whitespace(text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "\treturn cleaner.clean_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d51c8",
   "metadata": {},
   "source": [
    "## üî§ Classe TextTokenizer - Tokenisation\n",
    "\n",
    "Cette classe divise le texte nettoy√© en tokens (mots individuels) en utilisant le tokenizer de NLTK.\n",
    "\n",
    "**Configuration NLTK** :\n",
    "- Configure le chemin de donn√©es NLTK vers le dossier local\n",
    "- T√©l√©charge le package `punkt_tab` n√©cessaire pour la tokenisation\n",
    "\n",
    "**Fonctionnalit√©** :\n",
    "- Transforme une cha√Æne de caract√®res en liste de mots\n",
    "- G√®re les contractions et la ponctuation restante\n",
    "- Retourne une liste vide si l'entr√©e n'est pas valide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdd1bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Configure NLTK data path\n",
    "nltk.data.path.append(os.path.abspath(ntlk_dir))\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('punkt_tab', download_dir=ntlk_dir)\n",
    "\n",
    "class TextTokenizer:\n",
    "\t\"\"\"\n",
    "\tClass for tokenizing cleaned text into word-level tokens.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef tokenize(self, text) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tTokenize text into words.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The cleaned input text\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: List of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\treturn word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7345d4",
   "metadata": {},
   "source": [
    "## üö´ Classe StopwordRemover - Suppression des Mots Vides\n",
    "\n",
    "Cette classe supprime les mots vides (stopwords) qui n'apportent pas de sens s√©mantique significatif.\n",
    "\n",
    "**Caract√©ristiques** :\n",
    "- Utilise la liste de stopwords anglais de NLTK\n",
    "- **Pr√©servation des n√©gations** : Garde les mots comme \"not\", \"no\", \"never\", \"n't\" car ils sont cruciaux pour l'analyse de sentiment\n",
    "- Param√®tre de langue configurable (anglais par d√©faut)\n",
    "- Comparaison insensible √† la casse\n",
    "\n",
    "**Objectif** : R√©duire le bruit tout en pr√©servant les indicateurs de sentiment importants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cea26699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('stopwords', download_dir=ntlk_dir)\n",
    "\n",
    "class StopwordRemover:\n",
    "\t\"\"\"\n",
    "\tClass for removing stopwords from tokenized text.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, language=\"english\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the stopword remover with a given language.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): Language of the stopwords (default is 'english')\n",
    "\t\t\"\"\"\n",
    "\t\tself.stop_words = set(stopwords.words(language))\n",
    "\t\n",
    "\tdef remove_stopwords(self, tokens, keep_negation=True) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tRemove stopwords from a list of tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Tokens without stopwords\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\tif keep_negation:\n",
    "\t\t\tnegations = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words or word.lower() in negations]\n",
    "\t\telse:\n",
    "\t\t\treturn [word for word in tokens if word.lower() not in self.stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0463c5",
   "metadata": {},
   "source": [
    "## üîÑ Classe TextLemmatizer - Lemmatisation\n",
    "\n",
    "Cette classe r√©duit les mots √† leur forme canonique (lemme) pour normaliser les variations morphologiques.\n",
    "\n",
    "**Packages NLTK requis** :\n",
    "- `wordnet` : Base de donn√©es lexicale\n",
    "- `omw-1.4` : Open Multilingual Wordnet\n",
    "- `averaged_perceptron_tagger_eng` : √âtiqueteur morpho-syntaxique anglais\n",
    "\n",
    "**Fonctionnement** :\n",
    "1. **POS Tagging** : D√©termine la cat√©gorie grammaticale de chaque mot\n",
    "2. **Conversion des tags** : Convertit les tags TreeBank vers le format WordNet\n",
    "3. **Lemmatisation** : Applique la lemmatisation en fonction du type grammatical\n",
    "\n",
    "**Exemples** : \"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\", \"children\" ‚Üí \"child\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb654ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Comment once done for the first time\n",
    "# nltk.download('wordnet', download_dir=ntlk_dir)\n",
    "# nltk.download('omw-1.4', download_dir=ntlk_dir)\n",
    "# nltk.download('averaged_perceptron_tagger_eng', download_dir=ntlk_dir)\n",
    "\n",
    "class TextLemmatizer:\n",
    "\t\"\"\"\n",
    "\tClass for lemmatizing word tokens using NLTK's WordNetLemmatizer.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\t\n",
    "\tdef get_wordnet_pos(self, treebank_tag) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tConvert POS tag from Treebank to WordNet format for better lemmatization.\n",
    "\t\t\"\"\"\n",
    "\t\tif treebank_tag.startswith('J'):\n",
    "\t\t\treturn wordnet.ADJ\n",
    "\t\telif treebank_tag.startswith('V'):\n",
    "\t\t\treturn wordnet.VERB\n",
    "\t\telif treebank_tag.startswith('N'):\n",
    "\t\t\treturn wordnet.NOUN\n",
    "\t\telif treebank_tag.startswith('R'):\n",
    "\t\t\treturn wordnet.ADV\n",
    "\t\telse:\n",
    "\t\t\treturn wordnet.NOUN  # fallback\n",
    "\t\n",
    "\tdef lemmatize(self, tokens) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize a list of word tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Lemmatized tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)  # POS tagging\n",
    "\t\treturn [\n",
    "\t\t\tself.lemmatizer.lemmatize(token, self.get_wordnet_pos(pos))\n",
    "\t\t\tfor token, pos in pos_tags\n",
    "\t\t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b7d38",
   "metadata": {},
   "source": [
    "## üß™ D√©monstration du Pipeline Complet\n",
    "\n",
    "Cette section teste le pipeline complet sur des exemples vari√©s pour illustrer chaque √©tape du preprocessing.\n",
    "\n",
    "**Exemples de test** :\n",
    "1. **HTML + Mentions/URLs** : Texte avec balises, mentions et liens\n",
    "2. **Expressions informelles** : Contractions et ponctuation excessive  \n",
    "3. **N√©gations** : Test de pr√©servation des mots n√©gatifs\n",
    "4. **Contenu web** : Hashtags et URLs de sites web\n",
    "5. **Formes grammaticales** : Diff√©rentes conjugaisons et pluriels\n",
    "\n",
    "**√âtapes visualis√©es** :\n",
    "- Texte original ‚Üí Texte nettoy√© ‚Üí Tokens ‚Üí Sans stopwords ‚Üí Lemmatis√©\n",
    "\n",
    "Cela permet de v√©rifier l'efficacit√© de chaque composant du pipeline.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéõÔ∏è Approche 1 : Configuration par Param√®tres\n",
    "\n",
    "Cette approche utilise un dictionnaire de configuration pour contr√¥ler finement chaque √©tape du preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import emoji\n",
    "\n",
    "class FlexibleTextProcessor:\n",
    "\t\"\"\"\n",
    "\tFlexible text processor with configurable cleaning levels.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, config: Dict[str, Any] = None):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize with configuration dictionary.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tconfig: Dictionary with processing options\n",
    "\t\t\"\"\"\n",
    "\t\t# Default configuration (Light cleaning)\n",
    "\t\tself.default_config = {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': False,  # Keep hashtags for sentiment context\n",
    "\t\t\t'remove_punctuation': False,  # Keep punctuation for emphasis\n",
    "\t\t\t'remove_digits': False,  # Keep numbers for context\n",
    "\t\t\t'remove_emojis': False,  # Keep emojis for sentiment\n",
    "\t\t\t'expand_contractions': False,  # Keep natural speech patterns\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': False,  # Avoid over-normalization\n",
    "\t\t\t'preserve_caps': False,  # Keep CAPS for emphasis\n",
    "\t\t\t'min_word_length': 1,  # Keep short words like \"I\", \"no\"\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tself.config = self.default_config.copy()\n",
    "\t\tif config:\n",
    "\t\t\tself.config.update(config)\n",
    "\t\t\n",
    "\t\t# Initialize components\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\t\n",
    "\t\t# Contractions dictionary\n",
    "\t\tself.contractions = {\n",
    "\t\t\t\"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "\t\t\t\"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
    "\t\t\t\"'m\": \" am\", \"'s\": \" is\"\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_light_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Light cleaning configuration - minimal processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': False,\n",
    "\t\t\t'remove_punctuation': False,\n",
    "\t\t\t'remove_digits': False,\n",
    "\t\t\t'remove_emojis': False,\n",
    "\t\t\t'expand_contractions': False,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': False,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 1,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_medium_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Medium cleaning configuration - balanced processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': True,\n",
    "\t\t\t'remove_punctuation': True,\n",
    "\t\t\t'remove_digits': True,\n",
    "\t\t\t'remove_emojis': False,  # Keep emojis\n",
    "\t\t\t'expand_contractions': True,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': True,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 2,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_hard_config(self) -> Dict[str, Any]:\n",
    "\t\t\"\"\"Hard cleaning configuration - aggressive processing.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'remove_html': True,\n",
    "\t\t\t'to_lowercase': True,\n",
    "\t\t\t'remove_urls': True,\n",
    "\t\t\t'remove_mentions': True,\n",
    "\t\t\t'remove_hashtags': True,\n",
    "\t\t\t'remove_punctuation': True,\n",
    "\t\t\t'remove_digits': True,\n",
    "\t\t\t'remove_emojis': True,\n",
    "\t\t\t'expand_contractions': True,\n",
    "\t\t\t'normalize_whitespace': True,\n",
    "\t\t\t'remove_stopwords': True,\n",
    "\t\t\t'keep_negations': True,\n",
    "\t\t\t'apply_lemmatization': True,\n",
    "\t\t\t'preserve_caps': False,\n",
    "\t\t\t'min_word_length': 3,\n",
    "\t\t}\n",
    "\t\n",
    "\tdef expand_contractions(self, text: str) -> str:\n",
    "\t\t\"\"\"Expand contractions in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\tfor contraction, expansion in self.contractions.items():\n",
    "\t\t\ttext = text.replace(contraction, expansion)\n",
    "\t\treturn text\n",
    "\t\n",
    "\tdef remove_emojis(self, text: str) -> str:\n",
    "\t\t\"\"\"Remove emojis from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn emoji.replace_emoji(text, replace='')\n",
    "\t\n",
    "\tdef preserve_emphasis(self, text: str) -> str:\n",
    "\t\t\"\"\"Convert emphasis patterns to tokens.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Convert multiple punctuation to emphasis tokens\n",
    "\t\ttext = re.sub(r'!{2,}', ' VERY_EXCITED ', text)\n",
    "\t\ttext = re.sub(r'\\?{2,}', ' VERY_QUESTIONING ', text)\n",
    "\t\ttext = re.sub(r'\\.{3,}', ' TRAILING_THOUGHT ', text)\n",
    "\t\t\n",
    "\t\t# Convert CAPS words to emphasis tokens\n",
    "\t\tif not self.config.get('preserve_caps', False):\n",
    "\t\t\ttext = re.sub(r'\\b[A-Z]{2,}\\b', lambda m: f'EMPHASIS_{m.group().lower()}', text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\t\n",
    "\tdef process_text(self, text: str) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tProcess text according to configuration.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext: Input text to process\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tProcessed text string\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\tresult = text\n",
    "\t\t\n",
    "\t\t# Step 1: HTML removal\n",
    "\t\tif self.config.get('remove_html', True):\n",
    "\t\t\tresult = BeautifulSoup(result, \"html.parser\").get_text()\n",
    "\t\t\n",
    "\t\t# Step 2: Preserve emphasis before other processing\n",
    "\t\tif not self.config.get('remove_punctuation', True):\n",
    "\t\t\tresult = self.preserve_emphasis(result)\n",
    "\t\t\n",
    "\t\t# Step 3: Expand contractions\n",
    "\t\tif self.config.get('expand_contractions', False):\n",
    "\t\t\tresult = self.expand_contractions(result)\n",
    "\t\t\n",
    "\t\t# Step 4: Case handling\n",
    "\t\tif self.config.get('to_lowercase', True):\n",
    "\t\t\tif not self.config.get('preserve_caps', False):\n",
    "\t\t\t\tresult = result.lower()\n",
    "\t\t\n",
    "\t\t# Step 5: Remove various elements\n",
    "\t\tif self.config.get('remove_urls', True):\n",
    "\t\t\tresult = re.sub(r\"http\\S+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_mentions', True):\n",
    "\t\t\tresult = re.sub(r\"@\\w+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_hashtags', False):\n",
    "\t\t\tresult = re.sub(r\"#\\w+\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_emojis', False):\n",
    "\t\t\tresult = self.remove_emojis(result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_punctuation', False):\n",
    "\t\t\tresult = re.sub(r\"[^\\w\\s]\", \"\", result)\n",
    "\t\t\n",
    "\t\tif self.config.get('remove_digits', False):\n",
    "\t\t\tresult = re.sub(r\"\\d+\", \"\", result)\n",
    "\t\t\n",
    "\t\t# Step 6: Normalize whitespace\n",
    "\t\tif self.config.get('normalize_whitespace', True):\n",
    "\t\t\tresult = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "\t\t\n",
    "\t\treturn result\n",
    "\t\n",
    "\tdef process_tokens(self, tokens: list[str]) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tProcess tokens according to configuration.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens: List of tokens\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tProcessed list of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\t\n",
    "\t\tresult = tokens.copy()\n",
    "\t\t\n",
    "\t\t# Filter by minimum word length\n",
    "\t\tmin_length = self.config.get('min_word_length', 1)\n",
    "\t\tresult = [token for token in result if len(token) >= min_length]\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\tif self.config.get('remove_stopwords', True):\n",
    "\t\t\tkeep_neg = self.config.get('keep_negations', True)\n",
    "\t\t\tresult = self.stopword_remover.remove_stopwords(result, keep_negation=keep_neg)\n",
    "\t\t\n",
    "\t\t# Apply lemmatization\n",
    "\t\tif self.config.get('apply_lemmatization', False):\n",
    "\t\t\tresult = self.lemmatizer.lemmatize(result)\n",
    "\t\t\n",
    "\t\treturn result\n",
    "\t\n",
    "\tdef full_pipeline(self, text: str) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tComplete processing pipeline.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext: Input text\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList of processed tokens\n",
    "\t\t\"\"\"\n",
    "\t\t# Process text\n",
    "\t\tcleaned_text = self.process_text(text)\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned_text)\n",
    "\t\t\n",
    "\t\t# Process tokens\n",
    "\t\tfinal_tokens = self.process_tokens(tokens)\n",
    "\t\t\n",
    "\t\treturn final_tokens\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Approche 2 : Classes Sp√©cialis√©es (Enum-based)\n",
    "\n",
    "Cette approche utilise des enums et des classes sp√©cialis√©es pour chaque niveau de nettoyage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CleaningLevel(Enum):\n",
    "\tLIGHT = \"light\"\n",
    "\tMEDIUM = \"medium\"\n",
    "\tHARD = \"hard\"\n",
    "\tCUSTOM = \"custom\"\n",
    "\n",
    "class BaseProcessor(ABC):\n",
    "\t\"\"\"Abstract base class for text processors.\"\"\"\n",
    "\t\n",
    "\t@abstractmethod\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\tpass\n",
    "\n",
    "class LightProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tLight cleaning - Preserve sentiment indicators\n",
    "\t- Keep punctuation for emphasis (!!!, ???)\n",
    "\t- Keep emojis for emotional context\n",
    "\t- Keep contractions for natural speech\n",
    "\t- Minimal lemmatization to preserve meaning nuances\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Custom light cleaning\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\t\n",
    "\t\t# Remove only essential noise\n",
    "\t\tresult = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\tresult = result.lower()\n",
    "\t\tresult = re.sub(r\"http\\S+\", \"\", result)  # Remove URLs\n",
    "\t\tresult = re.sub(r\"@\\w+\", \"\", result)   # Remove mentions\n",
    "\t\t\n",
    "\t\t# Keep hashtags, punctuation, emojis, numbers\n",
    "\t\tresult = re.sub(r\"\\s+\", \" \", result).strip()\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(result)\n",
    "\t\t\n",
    "\t\t# Light stopword removal (keep negations)\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# No lemmatization to preserve word forms\n",
    "\t\treturn tokens\n",
    "\n",
    "class MediumProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tMedium cleaning - Balanced approach\n",
    "\t- Remove most punctuation but preserve key patterns\n",
    "\t- Selective emoji handling\n",
    "\t- Moderate lemmatization\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Standard cleaning\n",
    "\t\tcleaned = self.text_cleaner.clean_text(text)\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned)\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# Selective lemmatization (avoid over-normalization)\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)\n",
    "\t\tlemmatized = []\n",
    "\t\t\n",
    "\t\tfor token, pos in pos_tags:\n",
    "\t\t\t# Don't lemmatize adjectives to preserve sentiment intensity\n",
    "\t\t\tif pos.startswith('JJ'):  # Adjectives (better, worse, etc.)\n",
    "\t\t\t\tlemmatized.append(token)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemmatized.append(self.lemmatizer.lemmatizer.lemmatize(\n",
    "\t\t\t\t\ttoken, self.lemmatizer.get_wordnet_pos(pos)\n",
    "\t\t\t\t))\n",
    "\t\t\n",
    "\t\treturn lemmatized\n",
    "\n",
    "class HardProcessor(BaseProcessor):\n",
    "\t\"\"\"\n",
    "\tHard cleaning - Aggressive normalization\n",
    "\t- Remove all non-essential elements\n",
    "\t- Full lemmatization\n",
    "\t- Strict filtering\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.text_cleaner = TextCleaner()\n",
    "\t\tself.tokenizer = TextTokenizer()\n",
    "\t\tself.stopword_remover = StopwordRemover()\n",
    "\t\tself.lemmatizer = TextLemmatizer()\n",
    "\t\n",
    "\tdef process(self, text: str) -> list[str]:\n",
    "\t\t# Aggressive cleaning\n",
    "\t\tcleaned = self.text_cleaner.clean_text(text)\n",
    "\t\t\n",
    "\t\t# Additional aggressive steps\n",
    "\t\tcleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)  # Remove all punctuation\n",
    "\t\tcleaned = emoji.replace_emoji(cleaned, replace='')  # Remove emojis\n",
    "\t\t\n",
    "\t\t# Tokenize\n",
    "\t\ttokens = self.tokenizer.tokenize(cleaned)\n",
    "\t\t\n",
    "\t\t# Filter short tokens\n",
    "\t\ttokens = [token for token in tokens if len(token) >= 3]\n",
    "\t\t\n",
    "\t\t# Remove stopwords\n",
    "\t\ttokens = self.stopword_remover.remove_stopwords(tokens, keep_negation=True)\n",
    "\t\t\n",
    "\t\t# Full lemmatization\n",
    "\t\ttokens = self.lemmatizer.lemmatize(tokens)\n",
    "\t\t\n",
    "\t\treturn tokens\n",
    "\n",
    "class ProcessorFactory:\n",
    "\t\"\"\"Factory to create appropriate processor based on cleaning level.\"\"\"\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef create_processor(level: CleaningLevel) -> BaseProcessor:\n",
    "\t\t\"\"\"Create processor based on cleaning level.\"\"\"\n",
    "\t\tif level == CleaningLevel.LIGHT:\n",
    "\t\t\treturn LightProcessor()\n",
    "\t\telif level == CleaningLevel.MEDIUM:\n",
    "\t\t\treturn MediumProcessor()\n",
    "\t\telif level == CleaningLevel.HARD:\n",
    "\t\t\treturn HardProcessor()\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Unsupported cleaning level: {level}\")\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef process_text(text: str, level: CleaningLevel) -> list[str]:\n",
    "\t\t\"\"\"Quick processing with specified level.\"\"\"\n",
    "\t\tprocessor = ProcessorFactory.create_processor(level)\n",
    "\t\treturn processor.process(text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üß™ D√©monstration des Diff√©rentes Approches\n",
    "\n",
    "Comparaison des r√©sultats entre les diff√©rents niveaux de nettoyage sur les m√™mes exemples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2e2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92c8aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMONSTRATION OF THE PREPROCESSING ===\n",
      "\n",
      "üìù Exemple 1:\n",
      "Original: <p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\n",
      "Cleaned: 'hello check out this amazing tool'\n",
      "Tokens: ['hello', 'check', 'out', 'this', 'amazing', 'tool']\n",
      "Without stopwords: ['hello', 'check', 'amazing', 'tool']\n",
      "Lemmatized: ['hello', 'check', 'amaze', 'tool']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 2:\n",
      "Original: I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\n",
      "Cleaned: 'im loving the new chatgpt updates its so much better than before'\n",
      "Tokens: ['im', 'loving', 'the', 'new', 'chatgpt', 'updates', 'its', 'so', 'much', 'better', 'than', 'before']\n",
      "Without stopwords: ['im', 'loving', 'new', 'chatgpt', 'updates', 'much', 'better']\n",
      "Lemmatized: ['im', 'love', 'new', 'chatgpt', 'update', 'much', 'good']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 3:\n",
      "Original: Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\n",
      "Cleaned: 'why are people still using old technologies in makes no sense to me'\n",
      "Tokens: ['why', 'are', 'people', 'still', 'using', 'old', 'technologies', 'in', 'makes', 'no', 'sense', 'to', 'me']\n",
      "Without stopwords: ['people', 'still', 'using', 'old', 'technologies', 'makes', 'no', 'sense']\n",
      "Lemmatized: ['people', 'still', 'use', 'old', 'technology', 'make', 'no', 'sense']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 4:\n",
      "Original: <div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\n",
      "Cleaned: 'visit our website wwwexamplecom for more info about and'\n",
      "Tokens: ['visit', 'our', 'website', 'wwwexamplecom', 'for', 'more', 'info', 'about', 'and']\n",
      "Without stopwords: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "Lemmatized: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 5:\n",
      "Original: Running, jumped, better, good, children, mice, feet - testing different word forms\n",
      "Cleaned: 'running jumped better good children mice feet testing different word forms'\n",
      "Tokens: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Without stopwords: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Lemmatized: ['run', 'jump', 'good', 'good', 'child', 'mice', 'foot', 'test', 'different', 'word', 'form']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# Create the instances of the classes\n",
    "text_cleaner = TextCleaner()\n",
    "tokenizer = TextTokenizer()\n",
    "stopword_remover = StopwordRemover()\n",
    "lemmatizer = TextLemmatizer()\n",
    "\n",
    "# Examples of texts with different problems\n",
    "sample_texts = [\n",
    "\t\"<p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\",\n",
    "\t\"I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\",\n",
    "\t\"Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\",\n",
    "\t\"<div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\",\n",
    "\t\"Running, jumped, better, good, children, mice, feet - testing different word forms\"\n",
    "]\n",
    "\n",
    "print(\"=== DEMONSTRATION OF THE PREPROCESSING ===\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "\tprint(f\"üìù Exemple {i}:\")\n",
    "\tprint(f\"Original: {text}\")\n",
    "\t\n",
    "\t# Step 1: Cleaning\n",
    "\tcleaned = text_cleaner.clean_text(text)\n",
    "\tprint(f\"Cleaned: '{cleaned}'\")\n",
    "\t\n",
    "\t# Step 2: Tokenization\n",
    "\ttokens = tokenizer.tokenize(cleaned)\n",
    "\tprint(f\"Tokens: {tokens}\")\n",
    "\t\n",
    "\t# Step 3: Stopword Removal\n",
    "\ttokens_no_stop = stopword_remover.remove_stopwords(tokens)\n",
    "\tprint(f\"Without stopwords: {tokens_no_stop}\")\n",
    "\t\n",
    "\t# Step 4: Lemmatization\n",
    "\tlemmatized = lemmatizer.lemmatize(tokens_no_stop)\n",
    "\tprint(f\"Lemmatized: {lemmatized}\")\n",
    "\t\n",
    "\tprint(\"-\" * 80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
