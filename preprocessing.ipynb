{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42a42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/chatgpt-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaner:\n",
    "\t\"\"\"\n",
    "\tClass for cleaning and preprocessing text data with multiple processing steps.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef remove_html(self, text):\n",
    "\t\t\"\"\"Remove HTML tags from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\n",
    "\tdef convert_to_lowercase(self, text):\n",
    "\t\t\"\"\"Convert text to lowercase.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn text.lower()\n",
    "\t\n",
    "\tdef remove_urls(self, text):\n",
    "\t\t\"\"\"Remove URLs from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"http\\S+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_mentions_hashtags(self, text):\n",
    "\t\t\"\"\"Remove mentions (@username) and hashtags (#hashtag) from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_punctuation(self, text):\n",
    "\t\t\"\"\"Remove punctuation from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\n",
    "\tdef remove_digits(self, text):\n",
    "\t\t\"\"\"Remove digits from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\d+\", \"\", text)\n",
    "\t\n",
    "\tdef normalize_whitespace(self, text):\n",
    "\t\t\"\"\"Normalize whitespace in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\t\n",
    "\tdef clean_text(self, text):\n",
    "\t\t\"\"\"\n",
    "\t\tApply all cleaning steps to the text.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text to clean\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned text\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Apply all cleaning steps in sequence\n",
    "\t\ttext = self.remove_html(text)\n",
    "\t\ttext = self.convert_to_lowercase(text)\n",
    "\t\ttext = self.remove_urls(text)\n",
    "\t\ttext = self.remove_mentions_hashtags(text)\n",
    "\t\ttext = self.remove_punctuation(text)\n",
    "\t\ttext = self.remove_digits(text)\n",
    "\t\ttext = self.normalize_whitespace(text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\n",
    "# Create an instance for backward compatibility\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "def clean_text(text):\n",
    "\t\"\"\"\n",
    "\tLegacy function for backward compatibility.\n",
    "\t\"\"\"\n",
    "\treturn cleaner.clean_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd1bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Comment once done for the first time\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TextTokenizer:\n",
    "\t\"\"\"\n",
    "\tClass for tokenizing cleaned text into word-level tokens.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef tokenize(self, text):\n",
    "\t\t\"\"\"\n",
    "\t\tTokenize text into words.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The cleaned input text\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: List of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\treturn word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea26699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# À exécuter une seule fois dans ton code\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class StopwordRemover:\n",
    "\t\"\"\"\n",
    "\tClass for removing stopwords from tokenized text.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, language=\"english\"):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the stopword remover with a given language.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): Language of the stopwords (default is 'english')\n",
    "\t\t\"\"\"\n",
    "\t\tself.stop_words = set(stopwords.words(language))\n",
    "\t\n",
    "\tdef remove_stopwords(self, tokens):\n",
    "\t\t\"\"\"\n",
    "\t\tRemove stopwords from a list of tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Tokens without stopwords\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\treturn [word for word in tokens if word.lower() not in self.stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb654ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\omist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Comment once done for the first time\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class TextLemmatizer:\n",
    "\t\"\"\"\n",
    "\tClass for lemmatizing word tokens using NLTK's WordNetLemmatizer.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\t\n",
    "\tdef get_wordnet_pos(self, treebank_tag):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert POS tag from Treebank to WordNet format for better lemmatization.\n",
    "\t\t\"\"\"\n",
    "\t\tif treebank_tag.startswith('J'):\n",
    "\t\t\treturn wordnet.ADJ\n",
    "\t\telif treebank_tag.startswith('V'):\n",
    "\t\t\treturn wordnet.VERB\n",
    "\t\telif treebank_tag.startswith('N'):\n",
    "\t\t\treturn wordnet.NOUN\n",
    "\t\telif treebank_tag.startswith('R'):\n",
    "\t\t\treturn wordnet.ADV\n",
    "\t\telse:\n",
    "\t\t\treturn wordnet.NOUN  # fallback\n",
    "\t\n",
    "\tdef lemmatize(self, tokens):\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize a list of word tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Lemmatized tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)  # POS tagging\n",
    "\t\treturn [\n",
    "\t\t\tself.lemmatizer.lemmatize(token, self.get_wordnet_pos(pos))\n",
    "\t\t\tfor token, pos in pos_tags\n",
    "\t\t]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
