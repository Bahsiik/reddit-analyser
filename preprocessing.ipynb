{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d42a42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ntlk_dir = 'nltk_data'\n",
    "df = pd.read_csv('data/chatgpt-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "636445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaner:\n",
    "\t\"\"\"\n",
    "\tClass for cleaning and preprocessing text data with multiple processing steps.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef remove_html(self, text):\n",
    "\t\t\"\"\"Remove HTML tags from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\t\n",
    "\tdef convert_to_lowercase(self, text):\n",
    "\t\t\"\"\"Convert text to lowercase.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn text.lower()\n",
    "\t\n",
    "\tdef remove_urls(self, text):\n",
    "\t\t\"\"\"Remove URLs from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"http\\S+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_mentions_hashtags(self, text):\n",
    "\t\t\"\"\"Remove mentions (@username) and hashtags (#hashtag) from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\t\n",
    "\tdef remove_punctuation(self, text):\n",
    "\t\t\"\"\"Remove punctuation from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\n",
    "\tdef remove_digits(self, text):\n",
    "\t\t\"\"\"Remove digits from text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\d+\", \"\", text)\n",
    "\t\n",
    "\tdef normalize_whitespace(self, text):\n",
    "\t\t\"\"\"Normalize whitespace in text.\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\treturn re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\t\n",
    "\tdef clean_text(self, text):\n",
    "\t\t\"\"\"\n",
    "\t\tApply all cleaning steps to the text.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text to clean\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned text\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn \"\"\n",
    "\t\t\n",
    "\t\t# Apply all cleaning steps in sequence\n",
    "\t\ttext = self.remove_html(text)\n",
    "\t\ttext = self.convert_to_lowercase(text)\n",
    "\t\ttext = self.remove_urls(text)\n",
    "\t\ttext = self.remove_mentions_hashtags(text)\n",
    "\t\ttext = self.remove_punctuation(text)\n",
    "\t\ttext = self.remove_digits(text)\n",
    "\t\ttext = self.normalize_whitespace(text)\n",
    "\t\t\n",
    "\t\treturn text\n",
    "\n",
    "# Create an instance for backward compatibility\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "def clean_text(text):\n",
    "\t\"\"\"\n",
    "\tLegacy function for backward compatibility.\n",
    "\t\"\"\"\n",
    "\treturn cleaner.clean_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdd1bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Configure NLTK data path\n",
    "nltk.data.path.append(os.path.abspath(ntlk_dir))\n",
    "\n",
    "# Comment once done for the first time\n",
    "nltk.download('punkt_tab', download_dir=ntlk_dir)\n",
    "\n",
    "class TextTokenizer:\n",
    "\t\"\"\"\n",
    "\tClass for tokenizing cleaned text into word-level tokens.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef tokenize(self, text):\n",
    "\t\t\"\"\"\n",
    "\t\tTokenize text into words.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The cleaned input text\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: List of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(text, str):\n",
    "\t\t\treturn []\n",
    "\t\treturn word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cea26699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Comment once done for the first time\n",
    "nltk.download('stopwords', download_dir=ntlk_dir)\n",
    "\n",
    "class StopwordRemover:\n",
    "\t\"\"\"\n",
    "\tClass for removing stopwords from tokenized text.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, language=\"english\"):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the stopword remover with a given language.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): Language of the stopwords (default is 'english')\n",
    "\t\t\"\"\"\n",
    "\t\tself.stop_words = set(stopwords.words(language))\n",
    "\t\n",
    "\tdef remove_stopwords(self, tokens):\n",
    "\t\t\"\"\"\n",
    "\t\tRemove stopwords from a list of tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Tokens without stopwords\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\t\treturn [word for word in tokens if word.lower() not in self.stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb654ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Comment once done for the first time\n",
    "nltk.download('wordnet', download_dir=ntlk_dir)\n",
    "nltk.download('omw-1.4', download_dir=ntlk_dir)\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir=ntlk_dir)\n",
    "\n",
    "class TextLemmatizer:\n",
    "\t\"\"\"\n",
    "\tClass for lemmatizing word tokens using NLTK's WordNetLemmatizer.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\t\n",
    "\tdef get_wordnet_pos(self, treebank_tag):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert POS tag from Treebank to WordNet format for better lemmatization.\n",
    "\t\t\"\"\"\n",
    "\t\tif treebank_tag.startswith('J'):\n",
    "\t\t\treturn wordnet.ADJ\n",
    "\t\telif treebank_tag.startswith('V'):\n",
    "\t\t\treturn wordnet.VERB\n",
    "\t\telif treebank_tag.startswith('N'):\n",
    "\t\t\treturn wordnet.NOUN\n",
    "\t\telif treebank_tag.startswith('R'):\n",
    "\t\t\treturn wordnet.ADV\n",
    "\t\telse:\n",
    "\t\t\treturn wordnet.NOUN  # fallback\n",
    "\t\n",
    "\tdef lemmatize(self, tokens):\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize a list of word tokens.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\ttokens (List[str]): List of word tokens\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tList[str]: Lemmatized tokens\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(tokens, list):\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tpos_tags = nltk.pos_tag(tokens)  # POS tagging\n",
    "\t\treturn [\n",
    "\t\t\tself.lemmatizer.lemmatize(token, self.get_wordnet_pos(pos))\n",
    "\t\t\tfor token, pos in pos_tags\n",
    "\t\t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92c8aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âMONSTRATION DU PREPROCESSING ===\n",
      "\n",
      "üìù Exemple 1:\n",
      "Original: <p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\n",
      "Nettoy√©: 'hello check out this amazing tool'\n",
      "Tokens: ['hello', 'check', 'out', 'this', 'amazing', 'tool']\n",
      "Sans mots vides: ['hello', 'check', 'amazing', 'tool']\n",
      "Lemmatis√©: ['hello', 'check', 'amaze', 'tool']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 2:\n",
      "Original: I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\n",
      "Nettoy√©: 'im loving the new chatgpt updates its so much better than before'\n",
      "Tokens: ['im', 'loving', 'the', 'new', 'chatgpt', 'updates', 'its', 'so', 'much', 'better', 'than', 'before']\n",
      "Sans mots vides: ['im', 'loving', 'new', 'chatgpt', 'updates', 'much', 'better']\n",
      "Lemmatis√©: ['im', 'love', 'new', 'chatgpt', 'update', 'much', 'good']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 3:\n",
      "Original: Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\n",
      "Nettoy√©: 'why are people still using old technologies in makes no sense to me'\n",
      "Tokens: ['why', 'are', 'people', 'still', 'using', 'old', 'technologies', 'in', 'makes', 'no', 'sense', 'to', 'me']\n",
      "Sans mots vides: ['people', 'still', 'using', 'old', 'technologies', 'makes', 'sense']\n",
      "Lemmatis√©: ['people', 'still', 'use', 'old', 'technology', 'make', 'sense']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 4:\n",
      "Original: <div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\n",
      "Nettoy√©: 'visit our website wwwexamplecom for more info about and'\n",
      "Tokens: ['visit', 'our', 'website', 'wwwexamplecom', 'for', 'more', 'info', 'about', 'and']\n",
      "Sans mots vides: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "Lemmatis√©: ['visit', 'website', 'wwwexamplecom', 'info']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù Exemple 5:\n",
      "Original: Running, jumped, better, good, children, mice, feet - testing different word forms\n",
      "Nettoy√©: 'running jumped better good children mice feet testing different word forms'\n",
      "Tokens: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Sans mots vides: ['running', 'jumped', 'better', 'good', 'children', 'mice', 'feet', 'testing', 'different', 'word', 'forms']\n",
      "Lemmatis√©: ['run', 'jump', 'good', 'good', 'child', 'mice', 'foot', 'test', 'different', 'word', 'form']\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation avec des cha√Ænes de caract√®res al√©atoires\n",
    "\n",
    "# Cr√©er les instances des classes\n",
    "text_cleaner = TextCleaner()\n",
    "tokenizer = TextTokenizer()\n",
    "stopword_remover = StopwordRemover()\n",
    "lemmatizer = TextLemmatizer()\n",
    "\n",
    "# Exemples de textes avec diff√©rents probl√®mes\n",
    "sample_texts = [\n",
    "\t\"<p>Hello @user123! Check out this amazing #AI tool: https://example.com/awesome-tool üöÄ</p>\",\n",
    "\t\"I'm loving the new ChatGPT updates!!! It's so much better than before... üòç\",\n",
    "\t\"Why are people still using OLD technologies in 2024??? Makes NO sense to me!!!\",\n",
    "\t\"<div>Visit our website www.example.com for more info about #MachineLearning and #DataScience</div>\",\n",
    "\t\"Running, jumped, better, good, children, mice, feet - testing different word forms\"\n",
    "]\n",
    "\n",
    "print(\"=== D√âMONSTRATION DU PREPROCESSING ===\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "\tprint(f\"üìù Exemple {i}:\")\n",
    "\tprint(f\"Original: {text}\")\n",
    "\t\n",
    "\t# √âtape 1: Nettoyage\n",
    "\tcleaned = text_cleaner.clean_text(text)\n",
    "\tprint(f\"Nettoy√©: '{cleaned}'\")\n",
    "\t\n",
    "\t# √âtape 2: Tokenisation\n",
    "\ttokens = tokenizer.tokenize(cleaned)\n",
    "\tprint(f\"Tokens: {tokens}\")\n",
    "\t\n",
    "\t# √âtape 3: Suppression des mots vides\n",
    "\ttokens_no_stop = stopword_remover.remove_stopwords(tokens)\n",
    "\tprint(f\"Sans mots vides: {tokens_no_stop}\")\n",
    "\t\n",
    "\t# √âtape 4: Lemmatisation\n",
    "\tlemmatized = lemmatizer.lemmatize(tokens_no_stop)\n",
    "\tprint(f\"Lemmatis√©: {lemmatized}\")\n",
    "\t\n",
    "\tprint(\"-\" * 80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
