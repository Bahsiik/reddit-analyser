{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test du Pipeline Light de Preprocessing pour l'Analyse de Sentiment\n",
        "\n",
        "Ce notebook teste la version \"light\" complète du pipeline de preprocessing optimisée pour préserver les indicateurs émotionnels importants pour l'analyse de sentiment.\n",
        "\n",
        "## 📋 Pipeline testé :\n",
        "1. **Nettoyage** (`TextCleaner` light)\n",
        "2. **Tokenisation** (`TextTokenizer` amélioré)\n",
        "3. **Retrait des stopwords** (`StopwordRemover` light) \n",
        "4. **Lemmatisation** (`TextLemmatizer` light)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports des classes light\n",
        "import sys\n",
        "\n",
        "from preprocessing.light.text_cleaner_light import TextCleaner\n",
        "from preprocessing.text_tokenizer import TextTokenizer\n",
        "from preprocessing.light.stopword_remover_light import StopwordRemover\n",
        "from preprocessing.light.text_lemmatizer_light import TextLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration NLTK\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk_dir = 'nltk_data'\n",
        "nltk.data.path.append(os.path.abspath(nltk_dir))\n",
        "\n",
        "# Décommentez si premier run\n",
        "# nltk.download('punkt_tab', download_dir=nltk_dir)\n",
        "# nltk.download('stopwords', download_dir=nltk_dir)\n",
        "# nltk.download('wordnet', download_dir=nltk_dir)\n",
        "# nltk.download('omw-1.4', download_dir=nltk_dir)\n",
        "# nltk.download('averaged_perceptron_tagger_eng', download_dir=nltk_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Toutes les classes sont initialisées !\n"
          ]
        }
      ],
      "source": [
        "# Initialisation des classes\n",
        "text_cleaner = TextCleaner()\n",
        "tokenizer = TextTokenizer(nltk_dir=nltk_dir)\n",
        "stopword_remover = StopwordRemover()\n",
        "lemmatizer = TextLemmatizer(nltk_dir=nltk_dir)\n",
        "\n",
        "print(\"✅ Toutes les classes sont initialisées !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Exemples de Test Variés\n",
        "\n",
        "Nous allons tester différents types de textes pour voir comment le pipeline light préserve les éléments émotionnels :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 7 exemples de test préparés\n"
          ]
        }
      ],
      "source": [
        "# Exemples de textes avec différents défis pour l'analyse de sentiment\n",
        "test_texts = [\n",
        "\t\"I'm soooo HAPPY!!! This is absolutely AMAZING 😍 Much better than before!!!\",\n",
        "\t\n",
        "\t\"Can't believe how terrible this is... I'm extremely disappointed 😞 Worst experience EVER!\",\n",
        "\t\n",
        "\t\"<p>This ChatGPT update is @amazing #AI https://example.com BUT I don't think it's perfect yet...</p>\",\n",
        "\t\n",
        "\t\"Nooooo way! This is incredible!!! I absolutely looove it 💕 10/10 would recommend!\",\n",
        "\t\n",
        "\t\"It's quite good, but I've seen better products in 2024. Not bad though, just not outstanding.\",\n",
        "\t\n",
        "\t\"OMG this is HORRIBLE!!! Won't buy again, totally disgusting and revolting 🤮\",\n",
        "\t\n",
        "\t\"I really, really love this! It's so much more efficient than the old version. Fantastic work!\"\n",
        "]\n",
        "\n",
        "print(f\"📝 {len(test_texts)} exemples de test préparés\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Fonction de Pipeline Complète\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_text_light(text):\n",
        "\t\"\"\"\n",
        "\tApplique le pipeline light complet sur un texte.\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ttext (str): Texte à traiter\n",
        "\t\t\n",
        "\tReturns:\n",
        "\t\tdict: Résultats de chaque étape\n",
        "\t\"\"\"\n",
        "\tresults = {\n",
        "\t\t'original': text,\n",
        "\t\t'cleaned': '',\n",
        "\t\t'tokens': [],\n",
        "\t\t'without_stopwords': [],\n",
        "\t\t'lemmatized': []\n",
        "\t}\n",
        "\t\n",
        "\t# Étape 1: Nettoyage\n",
        "\tresults['cleaned'] = text_cleaner.clean_text(text)\n",
        "\t\n",
        "\t# Étape 2: Tokenisation\n",
        "\tresults['tokens'] = tokenizer.tokenize(results['cleaned'])\n",
        "\t\n",
        "\t# Étape 3: Retrait des stopwords\n",
        "\tresults['without_stopwords'] = stopword_remover.remove_stopwords(results['tokens'])\n",
        "\t\n",
        "\t# Étape 4: Lemmatisation\n",
        "\tresults['lemmatized'] = lemmatizer.lemmatize(results['without_stopwords'], conservative_mode=True)\n",
        "\t\n",
        "\treturn results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Test et Analyse des Résultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 1\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   I'm soooo HAPPY!!! This is absolutely AMAZING 😍 Much better than before!!!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'i'm soooo happy!!! this is absolutely AMAZING 😍 much better than before!!!'\n",
            "\n",
            "🔤 Tokens (18) :\n",
            "   [\"i'm\", 'soo', 'happy', '!', '!', '!', 'this', 'is', 'absolutely', 'AMAZING', '😍', 'much', 'better', 'than', 'before', '!', '!', '!']\n",
            "\n",
            "🚫 Sans stopwords (13) :\n",
            "   ['soo', 'happy', '!', '!', '!', 'absolutely', 'AMAZING', '😍', 'much', 'better', '!', '!', '!']\n",
            "\n",
            "🔄 Lemmatisé (13) :\n",
            "   ['soo', 'hapy', '!', '!', '!', 'absolutely', 'AMAZING', '😍', 'much', 'better', '!', '!', '!']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 2\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   Can't believe how terrible this is... I'm extremely disappointed 😞 Worst experience EVER!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'can't believe how terrible this is i'm extremely disappointed 😞 worst experience ever!'\n",
            "\n",
            "🔤 Tokens (14) :\n",
            "   [\"can't\", 'believe', 'how', 'terrible', 'this', 'is', \"i'm\", 'extremely', 'disappointed', '😞', 'worst', 'experience', 'ever', '!']\n",
            "\n",
            "🚫 Sans stopwords (10) :\n",
            "   [\"can't\", 'believe', 'terrible', 'extremely', 'disappointed', '😞', 'worst', 'experience', 'ever', '!']\n",
            "\n",
            "🔄 Lemmatisé (11) :\n",
            "   ['can', \"'t\", 'believe', 'terrible', 'extremely', 'disapointed', '😞', 'worst', 'experience', 'ever', '!']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 3\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   <p>This ChatGPT update is @amazing #AI https://example.com BUT I don't think it's perfect yet...</p>\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'this chatgpt update is BUT i don't think it's perfect yet'\n",
            "\n",
            "🔤 Tokens (11) :\n",
            "   ['this', 'chatgpt', 'update', 'is', 'BUT', 'i', \"don't\", 'think', \"it's\", 'perfect', 'yet']\n",
            "\n",
            "🚫 Sans stopwords (7) :\n",
            "   ['chatgpt', 'update', 'BUT', 'i', 'think', 'perfect', 'yet']\n",
            "\n",
            "🔄 Lemmatisé (7) :\n",
            "   ['chatgpt', 'update', 'BUT', 'i', 'think', 'perfect', 'yet']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 4\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   Nooooo way! This is incredible!!! I absolutely looove it 💕 10/10 would recommend!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'nooooo way! this is incredible!!! i absolutely looove it 💕 would recommend!'\n",
            "\n",
            "🔤 Tokens (17) :\n",
            "   ['noo', 'way', '!', 'this', 'is', 'incredible', '!', '!', '!', 'i', 'absolutely', 'loove', 'it', '💕', 'would', 'recommend', '!']\n",
            "\n",
            "🚫 Sans stopwords (14) :\n",
            "   ['noo', 'way', '!', 'incredible', '!', '!', '!', 'i', 'absolutely', 'loove', '💕', 'would', 'recommend', '!']\n",
            "\n",
            "🔄 Lemmatisé (14) :\n",
            "   ['noo', 'way', '!', 'incredible', '!', '!', '!', 'i', 'absolutely', 'love', '💕', 'would', 'recomend', '!']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 5\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   It's quite good, but I've seen better products in 2024. Not bad though, just not outstanding.\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'it's quite good but i've seen better products in not bad though just not outstanding'\n",
            "\n",
            "🔤 Tokens (15) :\n",
            "   [\"it's\", 'quite', 'good', 'but', \"i've\", 'seen', 'better', 'products', 'in', 'not', 'bad', 'though', 'just', 'not', 'outstanding']\n",
            "\n",
            "🚫 Sans stopwords (12) :\n",
            "   ['quite', 'good', 'but', 'seen', 'better', 'products', 'not', 'bad', 'though', 'just', 'not', 'outstanding']\n",
            "\n",
            "🔄 Lemmatisé (12) :\n",
            "   ['quite', 'god', 'but', 'sen', 'better', 'product', 'not', 'bad', 'though', 'just', 'not', 'outstanding']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 6\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   OMG this is HORRIBLE!!! Won't buy again, totally disgusting and revolting 🤮\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'OMG this is horrible!!! won't buy again totally disgusting and revolting 🤮'\n",
            "\n",
            "🔤 Tokens (15) :\n",
            "   ['OMG', 'this', 'is', 'horrible', '!', '!', '!', \"won't\", 'buy', 'again', 'totally', 'disgusting', 'and', 'revolting', '🤮']\n",
            "\n",
            "🚫 Sans stopwords (10) :\n",
            "   ['OMG', 'horrible', '!', '!', '!', 'buy', 'totally', 'disgusting', 'revolting', '🤮']\n",
            "\n",
            "🔄 Lemmatisé (10) :\n",
            "   ['OMG', 'horrible', '!', '!', '!', 'buy', 'totally', 'disgusting', 'revolting', '🤮']\n",
            "\n",
            "================================================================================\n",
            "📝 EXEMPLE 7\n",
            "================================================================================\n",
            "🔸 Original:\n",
            "   I really, really love this! It's so much more efficient than the old version. Fantastic work!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'i really really love this! it's so much more efficient than the old version fantastic work!'\n",
            "\n",
            "🔤 Tokens (18) :\n",
            "   ['i', 'really', 'really', 'love', 'this', '!', \"it's\", 'so', 'much', 'more', 'efficient', 'than', 'the', 'old', 'version', 'fantastic', 'work', '!']\n",
            "\n",
            "🚫 Sans stopwords (14) :\n",
            "   ['i', 'really', 'really', 'love', '!', 'so', 'much', 'more', 'efficient', 'old', 'version', 'fantastic', 'work', '!']\n",
            "\n",
            "🔄 Lemmatisé (14) :\n",
            "   ['i', 'realy', 'realy', 'love', '!', 'so', 'much', 'more', 'eficient', 'old', 'version', 'fantastic', 'work', '!']\n"
          ]
        }
      ],
      "source": [
        "def display_results(results, example_num):\n",
        "\t\"\"\"\n",
        "\tAffiche les résultats de façon lisible.\n",
        "\t\"\"\"\n",
        "\tprint(f\"\\n{'='*80}\")\n",
        "\tprint(f\"📝 EXEMPLE {example_num}\")\n",
        "\tprint(f\"{'='*80}\")\n",
        "\t\n",
        "\tprint(f\"🔸 Original:\")\n",
        "\tprint(f\"   {results['original']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🧹 Nettoyé:\")\n",
        "\tprint(f\"   '{results['cleaned']}'\")\n",
        "\t\n",
        "\tprint(f\"\\n🔤 Tokens ({len(results['tokens'])}) :\")\n",
        "\tprint(f\"   {results['tokens']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🚫 Sans stopwords ({len(results['without_stopwords'])}) :\")\n",
        "\tprint(f\"   {results['without_stopwords']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🔄 Lemmatisé ({len(results['lemmatized'])}) :\")\n",
        "\tprint(f\"   {results['lemmatized']}\")\n",
        "\n",
        "# Test de tous les exemples\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "\tresults = process_text_light(text)\n",
        "\tdisplay_results(results, i)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
