{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test du Pipeline Basic de Preprocessing\n",
        "\n",
        "Ce notebook teste la version \"basic\" du pipeline de preprocessing avec les classes standard.\n",
        "\n",
        "## 📋 Pipeline testé :\n",
        "1. **Nettoyage** (`TextCleaner` basic)\n",
        "2. **Tokenisation** (`TextTokenizer` amélioré)\n",
        "3. **Retrait des stopwords** (`StopwordRemover` basic) \n",
        "4. **Lemmatisation** (`TextLemmatizer` basic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports des classes basic\n",
        "import sys\n",
        "sys.path.append('preprocessing')\n",
        "sys.path.append('preprocessing/basic')\n",
        "\n",
        "from preprocessing.basic.text_cleaner import TextCleaner\n",
        "from preprocessing.text_tokenizer import TextTokenizer\n",
        "from preprocessing.basic.stopword_remover import StopwordRemover\n",
        "from preprocessing.basic.text_lemmatizer import TextLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration NLTK\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk_dir = 'nltk_data'\n",
        "nltk.data.path.append(os.path.abspath(nltk_dir))\n",
        "\n",
        "# Décommentez si premier run\n",
        "# nltk.download('punkt_tab', download_dir=nltk_dir)\n",
        "# nltk.download('stopwords', download_dir=nltk_dir)\n",
        "# nltk.download('wordnet', download_dir=nltk_dir)\n",
        "# nltk.download('omw-1.4', download_dir=nltk_dir)\n",
        "# nltk.download('averaged_perceptron_tagger_eng', download_dir=nltk_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Toutes les classes basic sont initialisées !\n"
          ]
        }
      ],
      "source": [
        "# Initialisation des classes\n",
        "text_cleaner = TextCleaner()\n",
        "tokenizer = TextTokenizer(nltk_dir=nltk_dir)\n",
        "stopword_remover = StopwordRemover()\n",
        "lemmatizer = TextLemmatizer()\n",
        "\n",
        "print(\"✅ Toutes les classes basic sont initialisées !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Exemples de Test\n",
        "\n",
        "Nous allons tester les mêmes exemples avec le pipeline basic :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 7 exemples de test préparés\n"
          ]
        }
      ],
      "source": [
        "# Exemples de textes avec différents défis\n",
        "test_texts = [\n",
        "\t\"I'm soooo HAPPY!!! This is absolutely AMAZING 😍 Much better than before!!!\",\n",
        "\t\n",
        "\t\"Can't believe how terrible this is... I'm extremely disappointed 😞 Worst experience EVER!\",\n",
        "\t\n",
        "\t\"<p>This ChatGPT update is @amazing #AI https://example.com BUT I don't think it's perfect yet...</p>\",\n",
        "\t\n",
        "\t\"Nooooo way! This is incredible!!! I absolutely looove it 💕 10/10 would recommend!\",\n",
        "\t\n",
        "\t\"It's quite good, but I've seen better products in 2024. Not bad though, just not outstanding.\",\n",
        "\t\n",
        "\t\"OMG this is HORRIBLE!!! Won't buy again, totally disgusting and revolting 🤮\",\n",
        "\t\n",
        "\t\"I really, really love this! It's so much more efficient than the old version. Fantastic work!\"\n",
        "]\n",
        "\n",
        "print(f\"📝 {len(test_texts)} exemples de test préparés\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Pipeline Basic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_text_basic(text):\n",
        "\t\"\"\"\n",
        "\tApplique le pipeline basic complet sur un texte.\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ttext (str): Texte à traiter\n",
        "\t\t\n",
        "\tReturns:\n",
        "\t\tdict: Résultats de chaque étape\n",
        "\t\"\"\"\n",
        "\tresults = {\n",
        "\t\t'original': text,\n",
        "\t\t'cleaned': '',\n",
        "\t\t'tokens': [],\n",
        "\t\t'without_stopwords': [],\n",
        "\t\t'lemmatized': []\n",
        "\t}\n",
        "\t\n",
        "\t# Étape 1: Nettoyage\n",
        "\tresults['cleaned'] = text_cleaner.clean_text(text)\n",
        "\t\n",
        "\t# Étape 2: Tokenisation\n",
        "\tresults['tokens'] = tokenizer.tokenize(results['cleaned'])\n",
        "\t\n",
        "\t# Étape 3: Retrait des stopwords\n",
        "\tresults['without_stopwords'] = stopword_remover.remove_stopwords(results['tokens'])\n",
        "\t\n",
        "\t# Étape 4: Lemmatisation\n",
        "\tresults['lemmatized'] = lemmatizer.lemmatize(results['without_stopwords'])\n",
        "\t\n",
        "\treturn results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Résultats du Pipeline Basic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 1\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   I'm soooo HAPPY!!! This is absolutely AMAZING 😍 Much better than before!!!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'im soooo happy this is absolutely amazing much better than before'\n",
            "\n",
            "🔤 Tokens (11) :\n",
            "   ['im', 'soo', 'happy', 'this', 'is', 'absolutely', 'amazing', 'much', 'better', 'than', 'before']\n",
            "\n",
            "🚫 Sans stopwords (7) :\n",
            "   ['im', 'soo', 'happy', 'absolutely', 'amazing', 'much', 'better']\n",
            "\n",
            "🔄 Lemmatisé (7) :\n",
            "   ['im', 'soo', 'happy', 'absolutely', 'amaze', 'much', 'well']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 2\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   Can't believe how terrible this is... I'm extremely disappointed 😞 Worst experience EVER!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'cant believe how terrible this is im extremely disappointed worst experience ever'\n",
            "\n",
            "🔤 Tokens (12) :\n",
            "   ['cant', 'believe', 'how', 'terrible', 'this', 'is', 'im', 'extremely', 'disappointed', 'worst', 'experience', 'ever']\n",
            "\n",
            "🚫 Sans stopwords (9) :\n",
            "   ['cant', 'believe', 'terrible', 'im', 'extremely', 'disappointed', 'worst', 'experience', 'ever']\n",
            "\n",
            "🔄 Lemmatisé (9) :\n",
            "   ['cant', 'believe', 'terrible', 'im', 'extremely', 'disappointed', 'bad', 'experience', 'ever']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 3\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   <p>This ChatGPT update is @amazing #AI https://example.com BUT I don't think it's perfect yet...</p>\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'this chatgpt update is but i dont think its perfect yet'\n",
            "\n",
            "🔤 Tokens (11) :\n",
            "   ['this', 'chatgpt', 'update', 'is', 'but', 'i', 'dont', 'think', 'its', 'perfect', 'yet']\n",
            "\n",
            "🚫 Sans stopwords (6) :\n",
            "   ['chatgpt', 'update', 'dont', 'think', 'perfect', 'yet']\n",
            "\n",
            "🔄 Lemmatisé (6) :\n",
            "   ['chatgpt', 'update', 'dont', 'think', 'perfect', 'yet']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 4\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   Nooooo way! This is incredible!!! I absolutely looove it 💕 10/10 would recommend!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'nooooo way this is incredible i absolutely looove it would recommend'\n",
            "\n",
            "🔤 Tokens (11) :\n",
            "   ['noo', 'way', 'this', 'is', 'incredible', 'i', 'absolutely', 'loove', 'it', 'would', 'recommend']\n",
            "\n",
            "🚫 Sans stopwords (7) :\n",
            "   ['noo', 'way', 'incredible', 'absolutely', 'loove', 'would', 'recommend']\n",
            "\n",
            "🔄 Lemmatisé (7) :\n",
            "   ['noo', 'way', 'incredible', 'absolutely', 'loove', 'would', 'recommend']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 5\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   It's quite good, but I've seen better products in 2024. Not bad though, just not outstanding.\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'its quite good but ive seen better products in not bad though just not outstanding'\n",
            "\n",
            "🔤 Tokens (15) :\n",
            "   ['its', 'quite', 'good', 'but', 'ive', 'seen', 'better', 'products', 'in', 'not', 'bad', 'though', 'just', 'not', 'outstanding']\n",
            "\n",
            "🚫 Sans stopwords (11) :\n",
            "   ['quite', 'good', 'ive', 'seen', 'better', 'products', 'not', 'bad', 'though', 'not', 'outstanding']\n",
            "\n",
            "🔄 Lemmatisé (11) :\n",
            "   ['quite', 'good', 'ive', 'see', 'well', 'product', 'not', 'bad', 'though', 'not', 'outstanding']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 6\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   OMG this is HORRIBLE!!! Won't buy again, totally disgusting and revolting 🤮\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'omg this is horrible wont buy again totally disgusting and revolting'\n",
            "\n",
            "🔤 Tokens (11) :\n",
            "   ['omg', 'this', 'is', 'horrible', 'wont', 'buy', 'again', 'totally', 'disgusting', 'and', 'revolting']\n",
            "\n",
            "🚫 Sans stopwords (7) :\n",
            "   ['omg', 'horrible', 'wont', 'buy', 'totally', 'disgusting', 'revolting']\n",
            "\n",
            "🔄 Lemmatisé (7) :\n",
            "   ['omg', 'horrible', 'wont', 'buy', 'totally', 'disgust', 'revolt']\n",
            "\n",
            "============================================================\n",
            "📝 EXEMPLE 7\n",
            "============================================================\n",
            "🔸 Original:\n",
            "   I really, really love this! It's so much more efficient than the old version. Fantastic work!\n",
            "\n",
            "🧹 Nettoyé:\n",
            "   'i really really love this its so much more efficient than the old version fantastic work'\n",
            "\n",
            "🔤 Tokens (16) :\n",
            "   ['i', 'really', 'really', 'love', 'this', 'its', 'so', 'much', 'more', 'efficient', 'than', 'the', 'old', 'version', 'fantastic', 'work']\n",
            "\n",
            "🚫 Sans stopwords (9) :\n",
            "   ['really', 'really', 'love', 'much', 'efficient', 'old', 'version', 'fantastic', 'work']\n",
            "\n",
            "🔄 Lemmatisé (9) :\n",
            "   ['really', 'really', 'love', 'much', 'efficient', 'old', 'version', 'fantastic', 'work']\n"
          ]
        }
      ],
      "source": [
        "def display_basic_results(results, example_num):\n",
        "\t\"\"\"\n",
        "\tAffiche les résultats de façon simple et lisible.\n",
        "\t\"\"\"\n",
        "\tprint(f\"\\n{'='*60}\")\n",
        "\tprint(f\"📝 EXEMPLE {example_num}\")\n",
        "\tprint(f\"{'='*60}\")\n",
        "\t\n",
        "\tprint(f\"🔸 Original:\")\n",
        "\tprint(f\"   {results['original']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🧹 Nettoyé:\")\n",
        "\tprint(f\"   '{results['cleaned']}'\")\n",
        "\t\n",
        "\tprint(f\"\\n🔤 Tokens ({len(results['tokens'])}) :\")\n",
        "\tprint(f\"   {results['tokens']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🚫 Sans stopwords ({len(results['without_stopwords'])}) :\")\n",
        "\tprint(f\"   {results['without_stopwords']}\")\n",
        "\t\n",
        "\tprint(f\"\\n🔄 Lemmatisé ({len(results['lemmatized'])}) :\")\n",
        "\tprint(f\"   {results['lemmatized']}\")\n",
        "\n",
        "# Test de tous les exemples avec le pipeline basic\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "\tresults = process_text_basic(text)\n",
        "\tdisplay_basic_results(results, i)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
