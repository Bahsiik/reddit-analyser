{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test des Algorithmes d'Embedding\n",
        "\n",
        "Ce notebook teste tous les algorithmes d'embedding disponibles dans le projet reddit-analyser.\n",
        "\n",
        "## Algorithmes testÃ©s :\n",
        "- **Bag of Words** (BoW)\n",
        "- **TF-IDF** \n",
        "- **Word2Vec**\n",
        "- **FastText**\n",
        "- **BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports nÃ©cessaires\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ajouter le dossier du projet au path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Imports des modules d'embedding\n",
        "from embedding.bag_of_words import BagOfWords\n",
        "from embedding.tf_idf import TfIdf\n",
        "from embedding.word2vec import Word2Vec\n",
        "from embedding.fasttext import FastText\n",
        "from embedding.bert import Bert\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š DonnÃ©es de test\n",
        "\n",
        "CrÃ©ons un jeu de donnÃ©es de test avec des commentaires Reddit simulÃ©s :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Nombre de textes de test : 10\n",
            "\n",
            "ğŸ” Exemples de textes :\n",
            "1. I love this subreddit! The community is amazing and very helpful.\n",
            "2. This is the worst post I've ever seen on Reddit. Complete garbage.\n",
            "3. Python is such a great programming language for data science.\n"
          ]
        }
      ],
      "source": [
        "# DonnÃ©es de test - commentaires Reddit simulÃ©s\n",
        "test_texts = [\n",
        "    \"I love this subreddit! The community is amazing and very helpful.\",\n",
        "    \"This is the worst post I've ever seen on Reddit. Complete garbage.\",\n",
        "    \"Python is such a great programming language for data science.\",\n",
        "    \"Machine learning and AI are revolutionizing technology today.\",\n",
        "    \"I hate when people don't use proper grammar in their posts.\",\n",
        "    \"The new update is fantastic! Really improved the user experience.\",\n",
        "    \"Can someone help me with this coding problem? I'm really stuck.\",\n",
        "    \"Data science requires strong skills in statistics and programming.\",\n",
        "    \"Reddit has some of the most interesting discussions online.\",\n",
        "    \"Natural language processing is a fascinating field of study.\"\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“ Nombre de textes de test : {len(test_texts)}\")\n",
        "print(\"\\nğŸ” Exemples de textes :\")\n",
        "for i, text in enumerate(test_texts[:3]):\n",
        "    print(f\"{i+1}. {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’ 1. Bag of Words (BoW)\n",
        "\n",
        "Test de l'algorithme Bag of Words :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’ Test de Bag of Words\n",
            "========================================\n",
            "âœ… SuccÃ¨s !\n",
            "ğŸ“ Forme des vecteurs : (10, 81)\n",
            "ğŸ“Š Taille du vocabulaire : 81\n",
            "\n",
            "ğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\n",
            "[0 0 1 1 0 0 0 1 0 0]\n",
            "\n",
            "ğŸ“ Exemples de mots dans le vocabulaire : ['a', 'ai', 'amazing', 'and', 'are', 'can', 'coding', 'community', 'complete', 'data']\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ’ Test de Bag of Words\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation\n",
        "    bow = BagOfWords()\n",
        "    \n",
        "    # Tokenisation simple des textes (BoW attend des listes de tokens)\n",
        "    tokenized_texts = [text.lower().split() for text in test_texts]\n",
        "    \n",
        "    # EntraÃ®nement sur les textes tokenisÃ©s\n",
        "    bow.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    bow_vectors = bow.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"âœ… SuccÃ¨s !\")\n",
        "    print(f\"ğŸ“ Forme des vecteurs : {bow_vectors.shape}\")\n",
        "    print(f\"ğŸ“Š Taille du vocabulaire : {len(bow.vocabulary_)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\nğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\")\n",
        "    print(bow_vectors[0][:10])\n",
        "    \n",
        "    # Afficher quelques mots du vocabulaire\n",
        "    vocab_sample = list(bow.vocabulary_.keys())[:10]\n",
        "    print(f\"\\nğŸ“ Exemples de mots dans le vocabulaire : {vocab_sample}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ˆ 2. TF-IDF\n",
        "\n",
        "Test de l'algorithme TF-IDF :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ˆ Test de TF-IDF\n",
            "========================================\n",
            "âœ… SuccÃ¨s !\n",
            "ğŸ“ Forme des vecteurs : (10, 81)\n",
            "ğŸ“Š Taille du vocabulaire : 81\n",
            "\n",
            "ğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\n",
            "[0.         0.         0.20932592 0.10945207 0.         0.\n",
            " 0.         0.20932592 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“ˆ Test de TF-IDF\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation\n",
        "    tfidf = TfIdf()\n",
        "    \n",
        "    # EntraÃ®nement sur les textes tokenisÃ©s\n",
        "    tfidf.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    tfidf_vectors = tfidf.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"âœ… SuccÃ¨s !\")\n",
        "    print(f\"ğŸ“ Forme des vecteurs : {tfidf_vectors.shape}\")\n",
        "    print(f\"ğŸ“Š Taille du vocabulaire : {len(tfidf.vocabulary_)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\nğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\")\n",
        "    print(tfidf_vectors[0][:10])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  3. Word2Vec\n",
        "\n",
        "Test de l'algorithme Word2Vec :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§  Test de Word2Vec\n",
            "========================================\n",
            "âœ… SuccÃ¨s !\n",
            "ğŸ“ Forme des vecteurs : (10, 50)\n",
            "ğŸ“Š Taille du vocabulaire : 81\n",
            "\n",
            "ğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\n",
            "[ 0.00066594 -0.00345364  0.00016632 -0.00100035 -0.00090419 -0.00395234\n",
            "  0.00197668  0.00182013 -0.0024775   0.00400689]\n",
            "\n",
            "ğŸ”— Mots similaires Ã  'python' : [('online.', 0.33277779817581177), ('experience.', 0.314900666475296), ('seen', 0.3062567710876465)]\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ§  Test de Word2Vec\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec les paramÃ¨tres supportÃ©s\n",
        "    w2v = Word2Vec(vector_size=50, min_count=1)\n",
        "    \n",
        "    # EntraÃ®nement sur les textes tokenisÃ©s\n",
        "    w2v.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    w2v_vectors = w2v.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"âœ… SuccÃ¨s !\")\n",
        "    print(f\"ğŸ“ Forme des vecteurs : {w2v_vectors.shape}\")\n",
        "    print(f\"ğŸ“Š Taille du vocabulaire : {len(w2v.model_.wv.key_to_index)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\nğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\")\n",
        "    print(w2v_vectors[0][:10])\n",
        "    \n",
        "    # Test de similaritÃ© entre mots\n",
        "    try:\n",
        "        similar_words = w2v.model_.wv.most_similar('python', topn=3)\n",
        "        print(f\"\\nğŸ”— Mots similaires Ã  'python' : {similar_words}\")\n",
        "    except:\n",
        "        print(\"\\nâš ï¸  Pas assez de donnÃ©es pour calculer les similaritÃ©s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš¡ 4. FastText\n",
        "\n",
        "Test de l'algorithme FastText :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš¡ Test de FastText\n",
            "========================================\n",
            "âœ… SuccÃ¨s !\n",
            "ğŸ“ Forme des vecteurs : (10, 50)\n",
            "ğŸ“Š Taille du vocabulaire : 81\n",
            "\n",
            "ğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\n",
            "[-1.1547178e-03 -1.0825418e-03 -2.0391089e-03  8.9013083e-05\n",
            "  1.1223384e-04  1.6854487e-03  7.6015288e-04 -1.2651983e-03\n",
            " -6.2090525e-04  3.2914043e-03]\n",
            "\n",
            "ğŸ†• FastText peut gÃ©rer les mots hors vocabulaire !\n",
            "ğŸ“ Taille du vecteur pour un mot inexistant : 50\n"
          ]
        }
      ],
      "source": [
        "print(\"âš¡ Test de FastText\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec les paramÃ¨tres supportÃ©s\n",
        "    ft = FastText(vector_size=50, min_count=1)\n",
        "    \n",
        "    # EntraÃ®nement sur les textes tokenisÃ©s\n",
        "    ft.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    ft_vectors = ft.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"âœ… SuccÃ¨s !\")\n",
        "    print(f\"ğŸ“ Forme des vecteurs : {ft_vectors.shape}\")\n",
        "    print(f\"ğŸ“Š Taille du vocabulaire : {len(ft.model_.wv.key_to_index)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\nğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\")\n",
        "    print(ft_vectors[0][:10])\n",
        "    \n",
        "    # Test avec un mot hors vocabulaire (avantage de FastText)\n",
        "    try:\n",
        "        oov_vector = ft.model_.wv['nonexistentword']\n",
        "        print(f\"\\nğŸ†• FastText peut gÃ©rer les mots hors vocabulaire !\")\n",
        "        print(f\"ğŸ“ Taille du vecteur pour un mot inexistant : {len(oov_vector)}\")\n",
        "    except:\n",
        "        print(\"\\nâš ï¸  Erreur avec les mots hors vocabulaire\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– 5. BERT\n",
        "\n",
        "Test de l'algorithme BERT :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Test de BERT\n",
            "========================================\n",
            "ğŸ“¥ Chargement du modÃ¨le BERT...\n",
            "âœ… SuccÃ¨s !\n",
            "ğŸ“ Forme des vecteurs : (3, 768)\n",
            "ğŸ¯ Dimension des embeddings BERT : 768\n",
            "\n",
            "ğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\n",
            "[ 0.12974845 -0.06577032 -0.03080706 -0.08123909 -0.08226358 -0.3270131\n",
            "  0.26065674  0.56690174 -0.23207587 -0.38512474]\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¤– Test de BERT\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec un modÃ¨le BERT lÃ©ger\n",
        "    bert = Bert(model_name='distilbert-base-uncased')\n",
        "    \n",
        "    # Charger le modÃ¨le prÃ©-entraÃ®nÃ© (BERT nÃ©cessite fit() pour initialiser)\n",
        "    print(\"ğŸ“¥ Chargement du modÃ¨le BERT...\")\n",
        "    bert.fit(test_texts[:3])  # Utiliser quelques textes pour initialiser\n",
        "    \n",
        "    # Transformation des textes en vecteurs (limitÃ© Ã  3 pour Ã©conomiser la mÃ©moire)\n",
        "    bert_vectors = bert.transform(test_texts[:3])\n",
        "    \n",
        "    print(f\"âœ… SuccÃ¨s !\")\n",
        "    print(f\"ğŸ“ Forme des vecteurs : {bert_vectors.shape}\")\n",
        "    print(f\"ğŸ¯ Dimension des embeddings BERT : {bert_vectors.shape[1]}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\nğŸ” Premier vecteur (premiers 10 Ã©lÃ©ments) :\")\n",
        "    print(bert_vectors[0][:10])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur : {e}\")\n",
        "    print(\"ğŸ’¡ BERT nÃ©cessite transformers et torch. Assure-toi qu'ils sont installÃ©s.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š 6. Comparaison des Algorithmes\n",
        "\n",
        "Comparons les performances et caractÃ©ristiques des diffÃ©rents algorithmes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Comparaison des Algorithmes\n",
            "==================================================\n",
            "  Algorithme  Dimension  Type PrÃ©-entraÃ®nÃ© Contexte\n",
            "Bag of Words         81 Dense          Non      Non\n",
            "      TF-IDF         81 Dense          Non      Non\n",
            "    Word2Vec         50 Dense          Non      Oui\n",
            "    FastText         50 Dense          Non      Oui\n",
            "        BERT        768 Dense          Oui      Oui\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“Š Comparaison des Algorithmes\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# CrÃ©er un DataFrame de comparaison\n",
        "comparison_data = []\n",
        "\n",
        "# Bag of Words\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'Bag of Words',\n",
        "        'Dimension': bow_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'PrÃ©-entraÃ®nÃ©': 'Non',\n",
        "        'Contexte': 'Non'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# TF-IDF\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'TF-IDF',\n",
        "        'Dimension': tfidf_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'PrÃ©-entraÃ®nÃ©': 'Non',\n",
        "        'Contexte': 'Non'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Word2Vec\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'Word2Vec',\n",
        "        'Dimension': w2v_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'PrÃ©-entraÃ®nÃ©': 'Non',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# FastText\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'FastText',\n",
        "        'Dimension': ft_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'PrÃ©-entraÃ®nÃ©': 'Non',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# BERT\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'BERT',\n",
        "        'Dimension': bert_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'PrÃ©-entraÃ®nÃ©': 'Oui',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Afficher le tableau de comparaison\n",
        "if comparison_data:\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "else:\n",
        "    print(\"âŒ Aucun algorithme n'a pu Ãªtre testÃ© avec succÃ¨s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ 7. Test de SimilaritÃ©\n",
        "\n",
        "Testons comment les diffÃ©rents algorithmes calculent la similaritÃ© entre textes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Test de SimilaritÃ© entre Textes\n",
            "==================================================\n",
            "ğŸ“ Textes de test :\n",
            "1. 'Python is great for data science'\n",
            "2. 'Data science requires Python programming'\n",
            "3. 'I hate this terrible post'\n",
            "\n",
            "ğŸ” TF-IDF :\n",
            "  SimilaritÃ© 1-2 (similaires): 0.981\n",
            "  SimilaritÃ© 1-3 (diffÃ©rents): 0.868\n",
            "  SimilaritÃ© 2-3 (diffÃ©rents): 0.949\n",
            "\n",
            "ğŸ” Word2Vec :\n",
            "  SimilaritÃ© 1-2 (similaires): 0.974\n",
            "  SimilaritÃ© 1-3 (diffÃ©rents): 0.805\n",
            "  SimilaritÃ© 2-3 (diffÃ©rents): 0.919\n",
            "\n",
            "ğŸ” FastText :\n",
            "  SimilaritÃ© 1-2 (similaires): 0.879\n",
            "  SimilaritÃ© 1-3 (diffÃ©rents): 0.823\n",
            "  SimilaritÃ© 2-3 (diffÃ©rents): 0.729\n",
            "\n",
            "ğŸ” BERT :\n",
            "  SimilaritÃ© 1-2 (similaires): 0.965\n",
            "  SimilaritÃ© 1-3 (diffÃ©rents): 0.910\n",
            "  SimilaritÃ© 2-3 (diffÃ©rents): 0.879\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¯ Test de SimilaritÃ© entre Textes\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prendre deux textes similaires et un diffÃ©rent\n",
        "text1 = \"Python is great for data science\"\n",
        "text2 = \"Data science requires Python programming\"\n",
        "text3 = \"I hate this terrible post\"\n",
        "\n",
        "test_similarity_texts = [text1, text2, text3]\n",
        "print(f\"ğŸ“ Textes de test :\")\n",
        "print(f\"1. '{text1}'\")\n",
        "print(f\"2. '{text2}'\")\n",
        "print(f\"3. '{text3}'\")\n",
        "print()\n",
        "\n",
        "# Fonction pour calculer la similaritÃ© cosinus\n",
        "def calculate_similarity(vectors, name):\n",
        "    try:\n",
        "        # Convertir en dense si sparse\n",
        "        if hasattr(vectors, 'toarray'):\n",
        "            vectors = vectors.toarray()\n",
        "        \n",
        "        sim_matrix = cosine_similarity(vectors)\n",
        "        \n",
        "        print(f\"ğŸ” {name} :\")\n",
        "        print(f\"  SimilaritÃ© 1-2 (similaires): {sim_matrix[0][1]:.3f}\")\n",
        "        print(f\"  SimilaritÃ© 1-3 (diffÃ©rents): {sim_matrix[0][2]:.3f}\")\n",
        "        print(f\"  SimilaritÃ© 2-3 (diffÃ©rents): {sim_matrix[1][2]:.3f}\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur pour {name}: {e}\")\n",
        "\n",
        "# Test avec chaque algorithme\n",
        "# TF-IDF\n",
        "try:\n",
        "    tfidf_sim_vectors = tfidf.transform(test_similarity_texts)\n",
        "    calculate_similarity(tfidf_sim_vectors, \"TF-IDF\")\n",
        "except:\n",
        "    print(\"âš ï¸  TF-IDF non disponible pour le test de similaritÃ©\")\n",
        "\n",
        "# Word2Vec\n",
        "try:\n",
        "    w2v_sim_vectors = w2v.transform(test_similarity_texts)\n",
        "    calculate_similarity(w2v_sim_vectors, \"Word2Vec\")\n",
        "except:\n",
        "    print(\"âš ï¸  Word2Vec non disponible pour le test de similaritÃ©\")\n",
        "\n",
        "# FastText\n",
        "try:\n",
        "    ft_sim_vectors = ft.transform(test_similarity_texts)\n",
        "    calculate_similarity(ft_sim_vectors, \"FastText\")\n",
        "except:\n",
        "    print(\"âš ï¸  FastText non disponible pour le test de similaritÃ©\")\n",
        "\n",
        "# BERT\n",
        "try:\n",
        "    bert_sim_vectors = bert.transform(test_similarity_texts)\n",
        "    calculate_similarity(bert_sim_vectors, \"BERT\")\n",
        "except:\n",
        "    print(\"âš ï¸  BERT non disponible pour le test de similaritÃ©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ 8. RÃ©sumÃ© et Recommandations\n",
        "\n",
        "Voici un rÃ©sumÃ© des caractÃ©ristiques de chaque algorithme :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’ **Bag of Words (BoW)**\n",
        "- âœ… **Avantages :** Simple, rapide, facile Ã  comprendre\n",
        "- âŒ **InconvÃ©nients :** Ignore l'ordre des mots, vecteurs trÃ¨s creux\n",
        "- ğŸ¯ **Usage :** Baseline, classification simple\n",
        "\n",
        "### ğŸ“ˆ **TF-IDF**\n",
        "- âœ… **Avantages :** AmÃ©liore BoW, donne plus d'importance aux mots rares\n",
        "- âŒ **InconvÃ©nients :** Toujours pas de contexte, vecteurs creux\n",
        "- ğŸ¯ **Usage :** Classification de texte, recherche d'information\n",
        "\n",
        "### ğŸ§  **Word2Vec**\n",
        "- âœ… **Avantages :** Capture le contexte, vecteurs denses, similaritÃ© sÃ©mantique\n",
        "- âŒ **InconvÃ©nients :** Moyenne les mots, problÃ¨me avec mots hors vocabulaire\n",
        "- ğŸ¯ **Usage :** Analyse sÃ©mantique, dÃ©tection de similaritÃ©\n",
        "\n",
        "### âš¡ **FastText**\n",
        "- âœ… **Avantages :** Comme Word2Vec + gÃ¨re les mots hors vocabulaire\n",
        "- âŒ **InconvÃ©nients :** Plus lourd, moins prÃ©cis sur certains cas\n",
        "- ğŸ¯ **Usage :** Quand il y a beaucoup de mots rares/nouveaux\n",
        "\n",
        "### ğŸ¤– **BERT**\n",
        "- âœ… **Avantages :** Ã‰tat de l'art, comprend le contexte bidirectionnel\n",
        "- âŒ **InconvÃ©nients :** TrÃ¨s lourd, lent, consomme beaucoup de mÃ©moire\n",
        "- ğŸ¯ **Usage :** TÃ¢ches complexes, quand la performance prime sur la vitesse\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ **Recommandations pour l'analyse de sentiment Reddit :**\n",
        "\n",
        "Pour notre projet d'analyse de sentiment sur Reddit, voici le plan d'action :\n",
        "\n",
        "### ğŸ¥‡ **Choix prioritaires : BERT ou FastText**\n",
        "- **BERT** sera probablement notre meilleur choix pour capturer le sarcasme, l'ironie et les nuances Ã©motionnelles complexes des commentaires Reddit\n",
        "- **FastText** comme alternative plus rapide, capable de gÃ©rer le slang et les nÃ©ologismes frÃ©quents sur Reddit\n",
        "\n",
        "### âŒ **Algorithmes inadaptÃ©s pour le sentiment :**\n",
        "- **TF-IDF et Bag of Words** sont vraiment **pas utilisables** pour l'analyse de sentiment car ils ignorent complÃ¨tement le contexte\n",
        "- Exemple : \"pas terrible\" serait mal classÃ© Ã  cause du mot \"pas\" analysÃ© sÃ©parÃ©ment\n",
        "\n",
        "### ğŸ§ª **StratÃ©gie de test :**\n",
        "MÃªme si on sait dÃ©jÃ  que BERT/FastText sont les plus prometteurs, **on testera quand mÃªme tous les algorithmes** une fois le modÃ¨le d'analyse de sentiment mis en place. Cela nous permettra de :\n",
        "- Confirmer nos hypothÃ¨ses avec des donnÃ©es rÃ©elles\n",
        "- Quantifier l'amÃ©lioration apportÃ©e par les algorithmes avancÃ©s\n",
        "- Avoir une baseline de comparaison solide\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
