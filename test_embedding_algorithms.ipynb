{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test des Algorithmes d'Embedding\n",
        "\n",
        "Ce notebook teste tous les algorithmes d'embedding disponibles dans le projet reddit-analyser.\n",
        "\n",
        "## Algorithmes testés :\n",
        "- **Bag of Words** (BoW)\n",
        "- **TF-IDF** \n",
        "- **Word2Vec**\n",
        "- **FastText**\n",
        "- **BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports nécessaires\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ajouter le dossier du projet au path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Imports des modules d'embedding\n",
        "from embedding.bag_of_words import BagOfWords\n",
        "from embedding.tf_idf import TfIdf\n",
        "from embedding.word2vec import Word2Vec\n",
        "from embedding.fasttext import FastText\n",
        "from embedding.bert import Bert\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Données de test\n",
        "\n",
        "Créons un jeu de données de test avec des commentaires Reddit simulés :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Nombre de textes de test : 10\n",
            "\n",
            "🔍 Exemples de textes :\n",
            "1. I love this subreddit! The community is amazing and very helpful.\n",
            "2. This is the worst post I've ever seen on Reddit. Complete garbage.\n",
            "3. Python is such a great programming language for data science.\n"
          ]
        }
      ],
      "source": [
        "# Données de test - commentaires Reddit simulés\n",
        "test_texts = [\n",
        "    \"I love this subreddit! The community is amazing and very helpful.\",\n",
        "    \"This is the worst post I've ever seen on Reddit. Complete garbage.\",\n",
        "    \"Python is such a great programming language for data science.\",\n",
        "    \"Machine learning and AI are revolutionizing technology today.\",\n",
        "    \"I hate when people don't use proper grammar in their posts.\",\n",
        "    \"The new update is fantastic! Really improved the user experience.\",\n",
        "    \"Can someone help me with this coding problem? I'm really stuck.\",\n",
        "    \"Data science requires strong skills in statistics and programming.\",\n",
        "    \"Reddit has some of the most interesting discussions online.\",\n",
        "    \"Natural language processing is a fascinating field of study.\"\n",
        "]\n",
        "\n",
        "print(f\"📝 Nombre de textes de test : {len(test_texts)}\")\n",
        "print(\"\\n🔍 Exemples de textes :\")\n",
        "for i, text in enumerate(test_texts[:3]):\n",
        "    print(f\"{i+1}. {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎒 1. Bag of Words (BoW)\n",
        "\n",
        "Test de l'algorithme Bag of Words :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎒 Test de Bag of Words\n",
            "========================================\n",
            "✅ Succès !\n",
            "📏 Forme des vecteurs : (10, 81)\n",
            "📊 Taille du vocabulaire : 81\n",
            "\n",
            "🔍 Premier vecteur (premiers 10 éléments) :\n",
            "[0 0 1 1 0 0 0 1 0 0]\n",
            "\n",
            "📝 Exemples de mots dans le vocabulaire : ['a', 'ai', 'amazing', 'and', 'are', 'can', 'coding', 'community', 'complete', 'data']\n"
          ]
        }
      ],
      "source": [
        "print(\"🎒 Test de Bag of Words\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation\n",
        "    bow = BagOfWords()\n",
        "    \n",
        "    # Tokenisation simple des textes (BoW attend des listes de tokens)\n",
        "    tokenized_texts = [text.lower().split() for text in test_texts]\n",
        "    \n",
        "    # Entraînement sur les textes tokenisés\n",
        "    bow.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    bow_vectors = bow.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"✅ Succès !\")\n",
        "    print(f\"📏 Forme des vecteurs : {bow_vectors.shape}\")\n",
        "    print(f\"📊 Taille du vocabulaire : {len(bow.vocabulary_)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\n🔍 Premier vecteur (premiers 10 éléments) :\")\n",
        "    print(bow_vectors[0][:10])\n",
        "    \n",
        "    # Afficher quelques mots du vocabulaire\n",
        "    vocab_sample = list(bow.vocabulary_.keys())[:10]\n",
        "    print(f\"\\n📝 Exemples de mots dans le vocabulaire : {vocab_sample}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 2. TF-IDF\n",
        "\n",
        "Test de l'algorithme TF-IDF :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📈 Test de TF-IDF\n",
            "========================================\n",
            "✅ Succès !\n",
            "📏 Forme des vecteurs : (10, 81)\n",
            "📊 Taille du vocabulaire : 81\n",
            "\n",
            "🔍 Premier vecteur (premiers 10 éléments) :\n",
            "[0.         0.         0.20932592 0.10945207 0.         0.\n",
            " 0.         0.20932592 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(\"📈 Test de TF-IDF\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation\n",
        "    tfidf = TfIdf()\n",
        "    \n",
        "    # Entraînement sur les textes tokenisés\n",
        "    tfidf.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    tfidf_vectors = tfidf.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"✅ Succès !\")\n",
        "    print(f\"📏 Forme des vecteurs : {tfidf_vectors.shape}\")\n",
        "    print(f\"📊 Taille du vocabulaire : {len(tfidf.vocabulary_)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\n🔍 Premier vecteur (premiers 10 éléments) :\")\n",
        "    print(tfidf_vectors[0][:10])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 3. Word2Vec\n",
        "\n",
        "Test de l'algorithme Word2Vec :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Test de Word2Vec\n",
            "========================================\n",
            "✅ Succès !\n",
            "📏 Forme des vecteurs : (10, 50)\n",
            "📊 Taille du vocabulaire : 81\n",
            "\n",
            "🔍 Premier vecteur (premiers 10 éléments) :\n",
            "[ 0.00066594 -0.00345364  0.00016632 -0.00100035 -0.00090419 -0.00395234\n",
            "  0.00197668  0.00182013 -0.0024775   0.00400689]\n",
            "\n",
            "🔗 Mots similaires à 'python' : [('online.', 0.33277779817581177), ('experience.', 0.314900666475296), ('seen', 0.3062567710876465)]\n"
          ]
        }
      ],
      "source": [
        "print(\"🧠 Test de Word2Vec\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec les paramètres supportés\n",
        "    w2v = Word2Vec(vector_size=50, min_count=1)\n",
        "    \n",
        "    # Entraînement sur les textes tokenisés\n",
        "    w2v.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    w2v_vectors = w2v.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"✅ Succès !\")\n",
        "    print(f\"📏 Forme des vecteurs : {w2v_vectors.shape}\")\n",
        "    print(f\"📊 Taille du vocabulaire : {len(w2v.model_.wv.key_to_index)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\n🔍 Premier vecteur (premiers 10 éléments) :\")\n",
        "    print(w2v_vectors[0][:10])\n",
        "    \n",
        "    # Test de similarité entre mots\n",
        "    try:\n",
        "        similar_words = w2v.model_.wv.most_similar('python', topn=3)\n",
        "        print(f\"\\n🔗 Mots similaires à 'python' : {similar_words}\")\n",
        "    except:\n",
        "        print(\"\\n⚠️  Pas assez de données pour calculer les similarités\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚡ 4. FastText\n",
        "\n",
        "Test de l'algorithme FastText :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚡ Test de FastText\n",
            "========================================\n",
            "✅ Succès !\n",
            "📏 Forme des vecteurs : (10, 50)\n",
            "📊 Taille du vocabulaire : 81\n",
            "\n",
            "🔍 Premier vecteur (premiers 10 éléments) :\n",
            "[-1.1547178e-03 -1.0825418e-03 -2.0391089e-03  8.9013083e-05\n",
            "  1.1223384e-04  1.6854487e-03  7.6015288e-04 -1.2651983e-03\n",
            " -6.2090525e-04  3.2914043e-03]\n",
            "\n",
            "🆕 FastText peut gérer les mots hors vocabulaire !\n",
            "📏 Taille du vecteur pour un mot inexistant : 50\n"
          ]
        }
      ],
      "source": [
        "print(\"⚡ Test de FastText\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec les paramètres supportés\n",
        "    ft = FastText(vector_size=50, min_count=1)\n",
        "    \n",
        "    # Entraînement sur les textes tokenisés\n",
        "    ft.fit(tokenized_texts)\n",
        "    \n",
        "    # Transformation des textes en vecteurs\n",
        "    ft_vectors = ft.transform(tokenized_texts)\n",
        "    \n",
        "    print(f\"✅ Succès !\")\n",
        "    print(f\"📏 Forme des vecteurs : {ft_vectors.shape}\")\n",
        "    print(f\"📊 Taille du vocabulaire : {len(ft.model_.wv.key_to_index)}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\n🔍 Premier vecteur (premiers 10 éléments) :\")\n",
        "    print(ft_vectors[0][:10])\n",
        "    \n",
        "    # Test avec un mot hors vocabulaire (avantage de FastText)\n",
        "    try:\n",
        "        oov_vector = ft.model_.wv['nonexistentword']\n",
        "        print(f\"\\n🆕 FastText peut gérer les mots hors vocabulaire !\")\n",
        "        print(f\"📏 Taille du vecteur pour un mot inexistant : {len(oov_vector)}\")\n",
        "    except:\n",
        "        print(\"\\n⚠️  Erreur avec les mots hors vocabulaire\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 5. BERT\n",
        "\n",
        "Test de l'algorithme BERT :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Test de BERT\n",
            "========================================\n",
            "📥 Chargement du modèle BERT...\n",
            "✅ Succès !\n",
            "📏 Forme des vecteurs : (3, 768)\n",
            "🎯 Dimension des embeddings BERT : 768\n",
            "\n",
            "🔍 Premier vecteur (premiers 10 éléments) :\n",
            "[ 0.12974845 -0.06577032 -0.03080706 -0.08123909 -0.08226358 -0.3270131\n",
            "  0.26065674  0.56690174 -0.23207587 -0.38512474]\n"
          ]
        }
      ],
      "source": [
        "print(\"🤖 Test de BERT\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Initialisation avec un modèle BERT léger\n",
        "    bert = Bert(model_name='distilbert-base-uncased')\n",
        "    \n",
        "    # Charger le modèle pré-entraîné (BERT nécessite fit() pour initialiser)\n",
        "    print(\"📥 Chargement du modèle BERT...\")\n",
        "    bert.fit(test_texts[:3])  # Utiliser quelques textes pour initialiser\n",
        "    \n",
        "    # Transformation des textes en vecteurs (limité à 3 pour économiser la mémoire)\n",
        "    bert_vectors = bert.transform(test_texts[:3])\n",
        "    \n",
        "    print(f\"✅ Succès !\")\n",
        "    print(f\"📏 Forme des vecteurs : {bert_vectors.shape}\")\n",
        "    print(f\"🎯 Dimension des embeddings BERT : {bert_vectors.shape[1]}\")\n",
        "    \n",
        "    # Exemple de vecteur\n",
        "    print(f\"\\n🔍 Premier vecteur (premiers 10 éléments) :\")\n",
        "    print(bert_vectors[0][:10])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur : {e}\")\n",
        "    print(\"💡 BERT nécessite transformers et torch. Assure-toi qu'ils sont installés.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 6. Comparaison des Algorithmes\n",
        "\n",
        "Comparons les performances et caractéristiques des différents algorithmes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Comparaison des Algorithmes\n",
            "==================================================\n",
            "  Algorithme  Dimension  Type Pré-entraîné Contexte\n",
            "Bag of Words         81 Dense          Non      Non\n",
            "      TF-IDF         81 Dense          Non      Non\n",
            "    Word2Vec         50 Dense          Non      Oui\n",
            "    FastText         50 Dense          Non      Oui\n",
            "        BERT        768 Dense          Oui      Oui\n"
          ]
        }
      ],
      "source": [
        "print(\"📊 Comparaison des Algorithmes\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Créer un DataFrame de comparaison\n",
        "comparison_data = []\n",
        "\n",
        "# Bag of Words\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'Bag of Words',\n",
        "        'Dimension': bow_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'Pré-entraîné': 'Non',\n",
        "        'Contexte': 'Non'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# TF-IDF\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'TF-IDF',\n",
        "        'Dimension': tfidf_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'Pré-entraîné': 'Non',\n",
        "        'Contexte': 'Non'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Word2Vec\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'Word2Vec',\n",
        "        'Dimension': w2v_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'Pré-entraîné': 'Non',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# FastText\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'FastText',\n",
        "        'Dimension': ft_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'Pré-entraîné': 'Non',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# BERT\n",
        "try:\n",
        "    comparison_data.append({\n",
        "        'Algorithme': 'BERT',\n",
        "        'Dimension': bert_vectors.shape[1],\n",
        "        'Type': 'Dense',\n",
        "        'Pré-entraîné': 'Oui',\n",
        "        'Contexte': 'Oui'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Afficher le tableau de comparaison\n",
        "if comparison_data:\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "else:\n",
        "    print(\"❌ Aucun algorithme n'a pu être testé avec succès\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 7. Test de Similarité\n",
        "\n",
        "Testons comment les différents algorithmes calculent la similarité entre textes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Test de Similarité entre Textes\n",
            "==================================================\n",
            "📝 Textes de test :\n",
            "1. 'Python is great for data science'\n",
            "2. 'Data science requires Python programming'\n",
            "3. 'I hate this terrible post'\n",
            "\n",
            "🔍 TF-IDF :\n",
            "  Similarité 1-2 (similaires): 0.981\n",
            "  Similarité 1-3 (différents): 0.868\n",
            "  Similarité 2-3 (différents): 0.949\n",
            "\n",
            "🔍 Word2Vec :\n",
            "  Similarité 1-2 (similaires): 0.974\n",
            "  Similarité 1-3 (différents): 0.805\n",
            "  Similarité 2-3 (différents): 0.919\n",
            "\n",
            "🔍 FastText :\n",
            "  Similarité 1-2 (similaires): 0.879\n",
            "  Similarité 1-3 (différents): 0.823\n",
            "  Similarité 2-3 (différents): 0.729\n",
            "\n",
            "🔍 BERT :\n",
            "  Similarité 1-2 (similaires): 0.965\n",
            "  Similarité 1-3 (différents): 0.910\n",
            "  Similarité 2-3 (différents): 0.879\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"🎯 Test de Similarité entre Textes\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prendre deux textes similaires et un différent\n",
        "text1 = \"Python is great for data science\"\n",
        "text2 = \"Data science requires Python programming\"\n",
        "text3 = \"I hate this terrible post\"\n",
        "\n",
        "test_similarity_texts = [text1, text2, text3]\n",
        "print(f\"📝 Textes de test :\")\n",
        "print(f\"1. '{text1}'\")\n",
        "print(f\"2. '{text2}'\")\n",
        "print(f\"3. '{text3}'\")\n",
        "print()\n",
        "\n",
        "# Fonction pour calculer la similarité cosinus\n",
        "def calculate_similarity(vectors, name):\n",
        "    try:\n",
        "        # Convertir en dense si sparse\n",
        "        if hasattr(vectors, 'toarray'):\n",
        "            vectors = vectors.toarray()\n",
        "        \n",
        "        sim_matrix = cosine_similarity(vectors)\n",
        "        \n",
        "        print(f\"🔍 {name} :\")\n",
        "        print(f\"  Similarité 1-2 (similaires): {sim_matrix[0][1]:.3f}\")\n",
        "        print(f\"  Similarité 1-3 (différents): {sim_matrix[0][2]:.3f}\")\n",
        "        print(f\"  Similarité 2-3 (différents): {sim_matrix[1][2]:.3f}\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur pour {name}: {e}\")\n",
        "\n",
        "# Test avec chaque algorithme\n",
        "# TF-IDF\n",
        "try:\n",
        "    tfidf_sim_vectors = tfidf.transform(test_similarity_texts)\n",
        "    calculate_similarity(tfidf_sim_vectors, \"TF-IDF\")\n",
        "except:\n",
        "    print(\"⚠️  TF-IDF non disponible pour le test de similarité\")\n",
        "\n",
        "# Word2Vec\n",
        "try:\n",
        "    w2v_sim_vectors = w2v.transform(test_similarity_texts)\n",
        "    calculate_similarity(w2v_sim_vectors, \"Word2Vec\")\n",
        "except:\n",
        "    print(\"⚠️  Word2Vec non disponible pour le test de similarité\")\n",
        "\n",
        "# FastText\n",
        "try:\n",
        "    ft_sim_vectors = ft.transform(test_similarity_texts)\n",
        "    calculate_similarity(ft_sim_vectors, \"FastText\")\n",
        "except:\n",
        "    print(\"⚠️  FastText non disponible pour le test de similarité\")\n",
        "\n",
        "# BERT\n",
        "try:\n",
        "    bert_sim_vectors = bert.transform(test_similarity_texts)\n",
        "    calculate_similarity(bert_sim_vectors, \"BERT\")\n",
        "except:\n",
        "    print(\"⚠️  BERT non disponible pour le test de similarité\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 8. Résumé et Recommandations\n",
        "\n",
        "Voici un résumé des caractéristiques de chaque algorithme :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎒 **Bag of Words (BoW)**\n",
        "- ✅ **Avantages :** Simple, rapide, facile à comprendre\n",
        "- ❌ **Inconvénients :** Ignore l'ordre des mots, vecteurs très creux\n",
        "- 🎯 **Usage :** Baseline, classification simple\n",
        "\n",
        "### 📈 **TF-IDF**\n",
        "- ✅ **Avantages :** Améliore BoW, donne plus d'importance aux mots rares\n",
        "- ❌ **Inconvénients :** Toujours pas de contexte, vecteurs creux\n",
        "- 🎯 **Usage :** Classification de texte, recherche d'information\n",
        "\n",
        "### 🧠 **Word2Vec**\n",
        "- ✅ **Avantages :** Capture le contexte, vecteurs denses, similarité sémantique\n",
        "- ❌ **Inconvénients :** Moyenne les mots, problème avec mots hors vocabulaire\n",
        "- 🎯 **Usage :** Analyse sémantique, détection de similarité\n",
        "\n",
        "### ⚡ **FastText**\n",
        "- ✅ **Avantages :** Comme Word2Vec + gère les mots hors vocabulaire\n",
        "- ❌ **Inconvénients :** Plus lourd, moins précis sur certains cas\n",
        "- 🎯 **Usage :** Quand il y a beaucoup de mots rares/nouveaux\n",
        "\n",
        "### 🤖 **BERT**\n",
        "- ✅ **Avantages :** État de l'art, comprend le contexte bidirectionnel\n",
        "- ❌ **Inconvénients :** Très lourd, lent, consomme beaucoup de mémoire\n",
        "- 🎯 **Usage :** Tâches complexes, quand la performance prime sur la vitesse\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Recommandations pour l'analyse de sentiment Reddit :**\n",
        "\n",
        "Pour notre projet d'analyse de sentiment sur Reddit, voici le plan d'action :\n",
        "\n",
        "### 🥇 **Choix prioritaires : BERT ou FastText**\n",
        "- **BERT** sera probablement notre meilleur choix pour capturer le sarcasme, l'ironie et les nuances émotionnelles complexes des commentaires Reddit\n",
        "- **FastText** comme alternative plus rapide, capable de gérer le slang et les néologismes fréquents sur Reddit\n",
        "\n",
        "### ❌ **Algorithmes inadaptés pour le sentiment :**\n",
        "- **TF-IDF et Bag of Words** sont vraiment **pas utilisables** pour l'analyse de sentiment car ils ignorent complètement le contexte\n",
        "- Exemple : \"pas terrible\" serait mal classé à cause du mot \"pas\" analysé séparément\n",
        "\n",
        "### 🧪 **Stratégie de test :**\n",
        "Même si on sait déjà que BERT/FastText sont les plus prometteurs, **on testera quand même tous les algorithmes** une fois le modèle d'analyse de sentiment mis en place. Cela nous permettra de :\n",
        "- Confirmer nos hypothèses avec des données réelles\n",
        "- Quantifier l'amélioration apportée par les algorithmes avancés\n",
        "- Avoir une baseline de comparaison solide\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
